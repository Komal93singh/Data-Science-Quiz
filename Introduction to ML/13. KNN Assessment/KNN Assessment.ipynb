{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b5fabc5",
   "metadata": {},
   "source": [
    "# Quiz : KNN Assessment\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a23211d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cfbc7db",
   "metadata": {},
   "source": [
    "## Q1. Which of the following is a limitation of k-d Trees? \n",
    "1. They cannot handle high-dimensional data \n",
    "2. They require more memory than brute force k-NN \n",
    "3. They perform poorly with uniformly distributed data \n",
    "4. Their performance degrades in very high dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eef297",
   "metadata": {},
   "source": [
    "The correct answers are:\n",
    "\n",
    "**1. They cannot handle high-dimensional data**\n",
    "**4. Their performance degrades in very high dimensions**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* k-d Trees work well for low to moderate dimensional data but struggle with high-dimensional data (the \"curse of dimensionality\").\n",
    "* As dimensions increase, the efficiency of k-d Trees drops significantly because the partitioning becomes less effective, causing most of the tree to be searched anyway.\n",
    "* They don't inherently require more memory than brute force k-NN; in fact, brute force uses less structure but more computation.\n",
    "* Uniformly distributed data does not necessarily cause poor performance; the main issue is high dimensionality.\n",
    "\n",
    "So, the key limitation is their poor scalability to high-dimensional spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f792d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7103bca",
   "metadata": {},
   "source": [
    "## Q2. In the context of k-NN regression, what does it mean if the algorithm suffers from high bias? \n",
    "1. The model is overfitting the training data \n",
    "2. The model is too complex and captures noise in the data \n",
    "3. The model is too simple and doesn't capture the underlying patterns well \n",
    "4. The model is highly sensitive to small changes in the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a4794e",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. The model is too simple and doesn't capture the underlying patterns well**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* High bias means the model is underfitting — it’s too simple to capture the true relationships in the data.\n",
    "* Overfitting and capturing noise (complex model) relates to **high variance**, not high bias.\n",
    "* Sensitivity to small input changes also points to high variance.\n",
    "\n",
    "So, in k-NN regression, if the algorithm has high bias, it usually means it’s not complex enough (for example, using too few neighbors or oversmoothing), thus missing important data patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f91a41",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afc8a9a9",
   "metadata": {},
   "source": [
    "## Q3. Which of the following is NOT a common technique to improve k-NN performance in high-dimensional spaces?\n",
    "1. Dimensionality reduction(e.g., PCA) \n",
    "2. Feature selection \n",
    "3. Using approximate k-NN algorithms \n",
    "4. Increasing the value of k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf87f9",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**4. Increasing the value of k**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* **Dimensionality reduction (PCA)** helps by reducing the number of features, making k-NN more efficient and effective in high dimensions.\n",
    "* **Feature selection** also reduces irrelevant or noisy features, improving performance.\n",
    "* **Approximate k-NN algorithms** speed up search in high dimensions by trading some accuracy for speed.\n",
    "* **Increasing k** does not directly solve the high-dimensionality problem; it may smooth predictions but doesn't address the curse of dimensionality or computational complexity.\n",
    "\n",
    "Thus, increasing k is **not** a common technique specifically aimed at improving k-NN performance in high-dimensional spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec819d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66ea7e16",
   "metadata": {},
   "source": [
    "## Q4. What is the main challenge in using k-NN with categorical variables? \n",
    "1. Categorical variables cannot be used in k-NN at all \n",
    "2. Defining an appropriate distance metric for categorical data \n",
    "3. Categorical variables always dominate the distance calaculation \n",
    "4. k-NN requires all variables to be continous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b92d4b",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Defining an appropriate distance metric for categorical data**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* k-NN relies on calculating distances between data points. For continuous variables, metrics like Euclidean distance work well.\n",
    "* For categorical variables, it’s not straightforward to measure \"distance\" because categories don’t have an inherent order or magnitude.\n",
    "* The main challenge is choosing or designing a suitable distance metric (like Hamming distance or using encoding techniques) that meaningfully compares categorical values.\n",
    "* It’s not true that categorical variables can’t be used at all or must be continuous, but the distance measure must be adapted.\n",
    "* Also, categorical variables don’t always dominate the distance unless improperly handled.\n",
    "\n",
    "So, the key issue is defining an appropriate distance metric for categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baad68c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12fab106",
   "metadata": {},
   "source": [
    "## Q5. In the context of k-NN, what does the term 'lazy learner' mean? \n",
    "1. The algorithm is slow to make predictions \n",
    "2. It doesn't require a training phase \n",
    "3. It only works with small datasets \n",
    "4. It uses minimal computational resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59382338",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. It doesn't require a training phase**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* A **lazy learner** means the algorithm essentially *waits* until a query is made to do most of its work (like computing distances and finding neighbors).\n",
    "* Unlike \"eager learners\" (e.g., decision trees, neural networks) that build a model during training, k-NN just stores the data and defers computation until prediction time.\n",
    "* It can be slow at prediction (not necessarily \"lazy\" in that sense), and it doesn't inherently require small datasets or minimal resources.\n",
    "\n",
    "So, \"lazy learner\" means **no explicit training phase, just storing data**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4866a78",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bc02614",
   "metadata": {},
   "source": [
    "## Q6. Which of the following is a potential drawback of using a large value of k in k-NN? \n",
    "1. Increased risk of overfitting \n",
    "2. Higher computational complexity \n",
    "3. Risk of underfitting \n",
    "4. Decreased ability to handle imbalanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aefb1de",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Risk of underfitting**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* A **large value of k** means the prediction is averaged over many neighbors, which can smooth out important local patterns and details, leading to **underfitting** (high bias).\n",
    "* Overfitting is more common with very small k (like k=1), where the model fits noise.\n",
    "* Computational complexity depends mostly on dataset size and method used, not directly on k.\n",
    "* Handling imbalanced datasets can be affected by k, but the primary drawback of large k is underfitting.\n",
    "\n",
    "So, the main drawback of large k is **risk of underfitting**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da5679",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59b765af",
   "metadata": {},
   "source": [
    "## Q7. In th econtext of k-D Trees, what does 'backtracking' refer to? \n",
    "1. Reversing the tree construction process \n",
    "2. Checking  other branches of the tree for potentially closer points \n",
    "3. Removing points from the tree \n",
    "4. Rebalancing the tree structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca81aca",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Checking other branches of the tree for potentially closer points**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* During a k-NN search using a k-d Tree, you traverse down to the leaf node based on splitting dimensions.\n",
    "* However, the nearest neighbor might not be in the same branch, so **backtracking** means going back up the tree and exploring other branches to find closer points that might have been missed.\n",
    "* It’s essential to ensure the true nearest neighbors are found.\n",
    "\n",
    "So, backtracking refers to **checking other branches of the tree for potentially closer points**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c3dcf4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9809d30",
   "metadata": {},
   "source": [
    "## Q8. Which of the following statements about k-NN is FALSE? \n",
    "1. It's sensitive to the scale of the features \n",
    "2. It doesn't require a training phase \n",
    "3. It's robust to outliers \n",
    "4. It can be used for both classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a210d027",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. It's robust to outliers**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* k-NN **is sensitive to outliers** because neighbors close to an outlier can skew predictions, especially if k is small.\n",
    "* It **is sensitive to feature scaling**, so normalization or standardization is important.\n",
    "* It **doesn't require a training phase** — it just stores data.\n",
    "* It **can be used for both classification and regression** tasks.\n",
    "\n",
    "So, the false statement is that k-NN is **robust to outliers**—it actually is **not** robust to them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f731246",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e36982b",
   "metadata": {},
   "source": [
    "## Q9. What is the primary advantage of using Locality-Sensitive Hashing (LSH) in approximate k-NN?\n",
    "1. It always finds the exact nearest neighbors \n",
    "2. It works well with categorical data \n",
    "3. It can quickly identify potential nearby points \n",
    "4. It reduces the memory required for storing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caba7f93",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. It can quickly identify potential nearby points**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Locality-Sensitive Hashing (LSH) is designed to **efficiently approximate nearest neighbors** by hashing data points so that nearby points are more likely to collide in the same bucket.\n",
    "* This enables **fast retrieval of candidate neighbors** without exhaustively searching the entire dataset.\n",
    "* It does **not guarantee exact nearest neighbors**, only approximate.\n",
    "* While LSH can be adapted for some data types, its main benefit is fast candidate retrieval, not specifically working better with categorical data or reducing memory.\n",
    "\n",
    "So, the primary advantage is that **LSH can quickly identify potential nearby points**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b472738f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b36355f",
   "metadata": {},
   "source": [
    "## Q'10. Which of the following is NOT a step in the k-D tree construction process? \n",
    "1. Selecting a dimension to split on \n",
    "2. Finding the median value in the selected dimension \n",
    "3. Partitioning the data based on the median \n",
    "4. Calculating the distance to all other points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb226f0c",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**4. Calculating the distance to all other points**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Building a k-d Tree involves:\n",
    "\n",
    "  1. Selecting a dimension to split on (usually cycling through dimensions).\n",
    "  2. Finding the median value in that dimension to split the data evenly.\n",
    "  3. Partitioning the data into left and right subtrees based on that median.\n",
    "* **Calculating distances to all other points** is **not** part of the tree construction; it’s done later during search/query.\n",
    "\n",
    "So, step 4 is **not** part of k-d Tree construction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f34031f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "967e21a2",
   "metadata": {},
   "source": [
    "## Q11. In k-NN classification, why is it common to choose an odd value for k? \n",
    "1. To improve accuracy \n",
    "2. To reduce computational complexity \n",
    "3. To avoid ties in voting \n",
    "4. To handle imbalanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f569518",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. To avoid ties in voting**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* In k-NN classification, the predicted class is decided by majority vote among the k nearest neighbors.\n",
    "* Choosing an **odd value for k** helps **avoid ties** when there are two classes (or generally, reduces the chances of tie votes).\n",
    "* It does not directly improve accuracy, reduce computational complexity, or specifically handle imbalanced datasets.\n",
    "\n",
    "So, the main reason is to **avoid ties in voting**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb8c7f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "738d80f6",
   "metadata": {},
   "source": [
    "## Q12. Which of the following best describes the 'curse of dimensionality' in the context of k-NN? \n",
    "1. The algorithm becomes slower as the number of features increase \n",
    "2. The distance between points becomes less meaningful in high-dimensional spaces \n",
    "3. More memory is requires to store high-dimensional data \n",
    "4. It's harder to visualilze high-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36f80a1",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. The distance between points becomes less meaningful in high-dimensional spaces**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* The **curse of dimensionality** refers to problems that arise when working with very high-dimensional data.\n",
    "* One key issue is that **distances between points tend to become very similar (less discriminative), making nearest neighbor search ineffective.**\n",
    "* While the algorithm can also slow down and memory usage can increase, the fundamental problem affecting k-NN is the loss of meaningful distance measures.\n",
    "* Visualization difficulty is true but is not the core issue for k-NN.\n",
    "\n",
    "So, the best description is that **distance between points becomes less meaningful in high dimensions**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20e2fd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40d30935",
   "metadata": {},
   "source": [
    "## Q13. What is the main advantage of Distance-Weighted k-NN over standard k-NN? \n",
    "1. It's faster to compute \n",
    "2. It handles categorical variables better \n",
    "3. It gives more impotance to closer neighbors \n",
    "4. It requires less memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d03490",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. It gives more importance to closer neighbors**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Distance-Weighted k-NN assigns weights to neighbors based on their distance, so **closer neighbors have a stronger influence** on the prediction than farther ones.\n",
    "* This often improves prediction quality compared to standard k-NN, which treats all neighbors equally.\n",
    "* It does not necessarily speed up computation, handle categorical variables better, or require less memory.\n",
    "\n",
    "So, the main advantage is **giving more importance to closer neighbors**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a441eb0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98bd9175",
   "metadata": {},
   "source": [
    "## Q14. Which of the following best describes k-Nearest Neighbors (k-NN)? \n",
    "1. A parametric supervised learning algorithm \n",
    "2. A non-parametric unsupervised learning algorithm \n",
    "3. A non-parameteric supervised leaning alegorthim \n",
    "4. A parametric unsupervised learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bfccad",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. A non-parametric supervised learning algorithm**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* k-NN is **supervised** because it uses labeled training data to make predictions.\n",
    "* It is **non-parametric** because it does not assume any fixed form or parameters for the underlying data distribution; it makes predictions based on the stored training data directly.\n",
    "* It is **not unsupervised** because it requires labels.\n",
    "* Parametric algorithms have a fixed number of parameters learned during training (like linear regression), which k-NN does not.\n",
    "\n",
    "So, k-NN is a **non-parametric supervised learning algorithm**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6588b734",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6b7e8eb",
   "metadata": {},
   "source": [
    "## Q15. What is the primary purpose of using aprroprimate k-NN algroithms? \n",
    "1. To improve accuracy \n",
    "2. To reduce memory usage \n",
    "3. To speed up the nearest neighbor search \n",
    "4. To handle categorical variables better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1abbd",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. To speed up the nearest neighbor search**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Approximate k-NN algorithms aim to **significantly reduce the time it takes to find neighbors** by sacrificing exactness for speed.\n",
    "* They don’t necessarily improve accuracy—in fact, they may slightly reduce it.\n",
    "* They don’t primarily focus on memory reduction or handling categorical variables better.\n",
    "\n",
    "So, the main goal is to **speed up nearest neighbor search**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95395fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9da17258",
   "metadata": {},
   "source": [
    "## Q16. Which of the following is NOT a common application of k-NN regression? \n",
    "1. House price prediction \n",
    "2. Stock price prediction \n",
    "3. Weather forecasting \n",
    "4. Image classfication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf0e258",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**4. Image classification**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* **k-NN regression** is used for predicting continuous values like house prices, stock prices, and weather variables.\n",
    "* **Image classification** is a **classification task**, not regression, so k-NN classification (not regression) would be used there.\n",
    "\n",
    "Thus, image classification is **not** a common application of k-NN regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51514a03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3349600",
   "metadata": {},
   "source": [
    "## Q17. In weighted k-NN, how are the weights typically assigned? \n",
    "1. Equally for all neighbors \n",
    "2. Inversley proportional to the distance from the query point \n",
    "3. Directly propoetional to the distance from the query point \n",
    "4. Randomly assigned]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea30b227",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Inversely proportional to the distance from the query point**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* In weighted k-NN, neighbors closer to the query point are given **higher weights**, so weights decrease as distance increases.\n",
    "* This means weights are typically assigned **inversely proportional to the distance** (e.g., weight = 1/distance).\n",
    "* Equal weighting is standard k-NN, not weighted.\n",
    "* Assigning weights directly proportional to distance or randomly would not make intuitive sense.\n",
    "\n",
    "So, weights are assigned **inversely proportional to distance**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d31833d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c487c4e2",
   "metadata": {},
   "source": [
    "## Q18. Which of the following is akey advantage of the k-D Tree over brute force k-NN? \n",
    "1. It always finds the xactt k nearest neghbors \n",
    "2. It reduces the search space, making it more efficient for high -dimensional data \n",
    "3. It works better with dynamic datasets \n",
    "4. It is simpler to implemet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d698b37",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**1. It always finds the exact k nearest neighbors**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* A k-D Tree is a spatial data structure that **enables faster nearest neighbor searches than brute force** by reducing the search space.\n",
    "* It **guarantees finding the exact k nearest neighbors**, unlike approximate methods.\n",
    "* However, it **does not work well for high-dimensional data** (its efficiency degrades as dimensionality increases).\n",
    "* It is **less suited for highly dynamic datasets** since updating the tree can be costly.\n",
    "* It is **more complex to implement** compared to brute force.\n",
    "\n",
    "So, the key advantage is **it finds the exact k nearest neighbors more efficiently** than brute force in low to moderate dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f604fd6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bd90d62",
   "metadata": {},
   "source": [
    "## Q19. What is the time complexity of the brute force k-NN algorithm for making a single prediction? \n",
    "1. o(1) \n",
    "2. o(log n) \n",
    "3. o(n) \n",
    "4. o(n log n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf9974",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. O(n)**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* In brute force k-NN, to predict for a single query point, you calculate the distance to **all n data points** in the dataset.\n",
    "* This results in a time complexity of **O(n)** per prediction.\n",
    "* There is no sorting or tree structure used to speed up the search in brute force.\n",
    "\n",
    "So, the time complexity for a single prediction in brute force k-NN is **O(n)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ac1bb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d126be7d",
   "metadata": {},
   "source": [
    "## Q20. Which of the following is NOT a common distance metric used in k-NN? \n",
    "1. Euclidean distance \n",
    "2. Manhattan distance \n",
    "3. Minkowski distance \n",
    "4. Gaussian distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2906ea",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**4. Gaussian distance**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* **Euclidean**, **Manhattan**, and **Minkowski** distances are common distance metrics used in k-NN.\n",
    "* **Gaussian distance** is not a standard distance metric; Gaussian usually refers to a kernel or probability distribution, not a direct distance measure.\n",
    "\n",
    "So, **Gaussian distance** is **not** a common distance metric in k-NN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1f725",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dae505ea",
   "metadata": {},
   "source": [
    "## Q21. What is the primary difference between k-NN classification and k-NN regression ? \n",
    "1. k-NN classification uses distance metrics , while k-NN regression doesn't \n",
    "2. k-NN classification predicts discrete class labels, ehile k-NN regression predicts continuoys values \n",
    "3. k-NN classification requires scaling , while K-NN regression doesn't \n",
    "4. K-NN classification uses odd values of k, while k-NN regression uses even va;lues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0ac92f",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. k-NN classification predicts discrete class labels, while k-NN regression predicts continuous values**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* k-NN **classification** assigns a class label based on the majority vote of nearest neighbors.\n",
    "* k-NN **regression** predicts a continuous output by averaging the values of nearest neighbors.\n",
    "* Both use distance metrics and usually require scaling.\n",
    "* The choice of odd or even k is a convention mostly for classification to avoid ties, not a strict difference.\n",
    "\n",
    "So, the primary difference is in the **type of output predicted: discrete classes vs. continuous values**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c0636b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89ba03d4",
   "metadata": {},
   "source": [
    "## Q22. What is the primary difference between Ball Trees and K-D Trees? \n",
    "1. Ball Trees use spherical partitions, while k-D trees use hyperplane partitions \n",
    "2. Ball Trees can only handle 3D data, while k-D trees work in any dimension \n",
    "3. Ball Trees are faster to construct , but slower to query \n",
    "4. Ball Trees require less memory than k-D Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25c8291",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**1. Ball Trees use spherical partitions, while k-D Trees use hyperplane partitions**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* **Ball Trees** partition data space into nested hyperspheres (\"balls\").\n",
    "* **k-D Trees** partition data space using axis-aligned hyperplanes (splitting on one dimension at a time).\n",
    "* Ball Trees can handle any dimension, not just 3D.\n",
    "* Construction and query times depend on data and implementation, not necessarily fitting options 3 or 4.\n",
    "\n",
    "So, the primary difference is the type of partitioning: **spherical vs. hyperplane**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9de58f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ee6b919",
   "metadata": {},
   "source": [
    "## Q23. Which of the following scenarios would likely benefit most from using approximate k_NN instead of exact k-NN? \n",
    "1. A small dataset with high -dimensional features \n",
    "2. A large dataset where speed is crucial, and slight inaccuracies are tolerable \n",
    "3. A dataset with many categorical variables \n",
    "4. A regression task with a small number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b0bff7",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. A large dataset where speed is crucial, and slight inaccuracies are tolerable**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Approximate k-NN algorithms are designed to **speed up neighbor searches**, especially in large datasets where exact k-NN would be too slow.\n",
    "* They trade some accuracy for much faster query times, which is acceptable when slight inaccuracies don’t harm results much.\n",
    "* Small datasets or low-dimensional regression tasks don’t usually need approximate methods.\n",
    "* Handling categorical variables is not the main strength of approximate k-NN.\n",
    "\n",
    "So, approximate k-NN benefits **large datasets with a need for speed and tolerance for small errors**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b29d2a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7308b1df",
   "metadata": {},
   "source": [
    "## Q24. In the context of k-D Trees, what is the significance of the median value when splitting a dimension? \n",
    "1. It ensures the tree is perfectly balanced \n",
    "2. It minimizes the depth of the tree \n",
    "3. It guarantees optimal search performance \n",
    "4. It helps create a relatively balanced partition of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e71407a",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**4. It helps create a relatively balanced partition of the data**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Choosing the **median** value for splitting ensures that roughly half the points go to one side and half to the other, resulting in a **relatively balanced tree**.\n",
    "* This helps improve search efficiency, but the tree may not be perfectly balanced.\n",
    "* It does not guarantee optimal search performance but generally helps avoid extremely unbalanced trees.\n",
    "\n",
    "So, the median split helps create a **relatively balanced partition**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b74b8a0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e958e9ae",
   "metadata": {},
   "source": [
    "## Q25. What is the primary difference between brute force k-NN and tree based k-NN implementations in terms of their suitability for dynamic datasets? \n",
    "1. Brute force k-NN handles dynamic datasets better \n",
    "2. Tree-based methods are always faster for insertions and deletions \n",
    "3. Brute force k-NN requires recomputation of all distances for any change \n",
    "4. Tree-based methods don't support dynamic datasets at all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0d2769",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**1. Brute force k-NN handles dynamic datasets better**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* **Brute force k-NN** simply stores all data points, so adding or removing points is straightforward (just add or remove data without rebuilding anything).\n",
    "* **Tree-based methods (like k-D Trees)** often require complex and costly updates or even rebuilding the tree to maintain balance after insertions/deletions, making them less suitable for frequently changing datasets.\n",
    "* Brute force doesn't require recomputing distances until query time, but that’s not the main factor for dynamic data handling.\n",
    "* Tree-based methods *do* support dynamic data, but with overhead.\n",
    "\n",
    "So, brute force k-NN is generally better for dynamic datasets because it handles insertions/deletions more easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2932338",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11778ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c908e6f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8cbf18",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
