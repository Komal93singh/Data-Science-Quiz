{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d04eb0",
   "metadata": {},
   "source": [
    "# QUIZ : Boosting Assessment\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede79bbe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e66dcef",
   "metadata": {},
   "source": [
    "## Q1. What is the main purpose of boosting in machine learning? \n",
    "1. To reduce the complexity of the model \n",
    "2. TO increase the accuracy of a model by combining multiple weak learners \n",
    "3. To decrease the computational cost of training \n",
    "4. To perform feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047872c4",
   "metadata": {},
   "source": [
    "The main purpose of boosting in machine learning is:\n",
    "\n",
    "**2. To increase the accuracy of a model by combining multiple weak learners**\n",
    "\n",
    "Boosting works by sequentially training weak learners (usually simple models) where each subsequent model focuses on correcting the errors of the previous ones. This leads to a strong, accurate combined model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeedd73",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "872ce8a9",
   "metadata": {},
   "source": [
    "## Q2. How does XGBoost handle missing values during training? \n",
    "1. By discarding data with missing values \n",
    "2. By imputing missing values before training \n",
    "3. By learning the best way to handle missing values directly \n",
    "4. By using a different model for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a0e11",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. By learning the best way to handle missing values directly**\n",
    "\n",
    "XGBoost automatically learns the optimal direction to send missing values in each tree split during training, so it doesn’t require explicit imputation beforehand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ef7cc6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "037b38c1",
   "metadata": {},
   "source": [
    "## Q3. What is a common disadvantage of Gradient Boosting ? \n",
    "1. It is prone to overfitting with noisy data \n",
    "2. It is computationally intensive and requires significant training time \n",
    "3. It cannot handle large datasets effectively \n",
    "4. It provides less accurate results compared to other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d333710",
   "metadata": {},
   "source": [
    "The best answer is:\n",
    "\n",
    "**1. It is prone to overfitting with noisy data**\n",
    "\n",
    "Gradient Boosting models, especially if not properly regularized, can overfit noisy data. While it can be computationally intensive (option 2), the primary commonly cited disadvantage is overfitting on noisy or small datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e6932",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4030b0e",
   "metadata": {},
   "source": [
    "## Q4. What is the purpose of 'loss function' in boosting methods? \n",
    "1. To evaluate the performance of the model \n",
    "2. To measure how well predictions match actual outcomes and guide model improvement \n",
    "3. To select features for training \n",
    "4. To combine predictions from multiple models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d7ef4",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. To measure how well predictions match actual outcomes and guide model improvement**\n",
    "\n",
    "In boosting, the loss function quantifies the error of predictions compared to actual values and helps the algorithm focus on reducing this error in subsequent iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ad86c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03e76633",
   "metadata": {},
   "source": [
    "## Q5. Which of the following is true about the use of weak learners in boosting? \n",
    "1. They must be complex and high-performing models \n",
    "2. They are simple models that, when combined, improved overall performance \n",
    "3. They are used only for initial data exploration \n",
    "4. They need to be independent of each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339471c7",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. They are simple models that, when combined, improve overall performance**\n",
    "\n",
    "Weak learners in boosting are typically simple models (like shallow decision trees) that individually perform just slightly better than random guessing, but combined sequentially, they create a strong, accurate model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a15ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11f48737",
   "metadata": {},
   "source": [
    "## Q6. Which boosting technique is most likely to be used for feature importance evaluation? \n",
    "1. AdaBoost \n",
    "2. Gradient Boosting \n",
    "3. XGBoost \n",
    "4. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca443d8b",
   "metadata": {},
   "source": [
    "The best answer is:\n",
    "\n",
    "**3. XGBoost**\n",
    "\n",
    "XGBoost is widely used not only for its boosting capabilities but also because it provides detailed and reliable feature importance metrics, making it popular for feature importance evaluation. While Gradient Boosting also provides feature importance, XGBoost is especially known for its efficiency and advanced options. Random Forest (option 4) is not a boosting technique; it’s an ensemble of decision trees using bagging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5614cfac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "300c858f",
   "metadata": {},
   "source": [
    "## Q7. What is the primary advantage of Gradient Boosting over AdaBoost? \n",
    "1. Better handling of large datasets \n",
    "2. Greater robustness to noisy data \n",
    "3. Simpler implementation \n",
    "4. Faster training time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6720a3",
   "metadata": {},
   "source": [
    "The primary advantage of Gradient Boosting over AdaBoost is:\n",
    "\n",
    "**2. Greater robustness to noisy data**\n",
    "\n",
    "Gradient Boosting uses a loss function that can be tailored and optimizes predictions more flexibly, making it generally more robust to noise compared to AdaBoost, which can be more sensitive to outliers and noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ba4eda",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "017de818",
   "metadata": {},
   "source": [
    "## Q8. Which of the following best describes a 'weak learner'? \n",
    "1. A model that performs very well on its own \n",
    "2. A model with low predictive power taht can be improved when others \n",
    "3. A model that cannot handle non-linear relationships \n",
    "4. A model that is used only for data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e820f0b",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. A model with low predictive power that can be improved when combined with others**\n",
    "\n",
    "A weak learner is a simple model that performs just slightly better than random guessing but, when combined with others (like in boosting), creates a strong, accurate overall model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bbc8d5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "019218c1",
   "metadata": {},
   "source": [
    "## Q9. What does 'ensemble methods' refer to in boosting? \n",
    "1. Combining multiple models to improve performance \n",
    "2. Using a single powerful model to achieve high accuracy \n",
    "3. Selecting th ebest features for training \n",
    "4. Applying different algorithms to the same dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1de8600",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**1. Combining multiple models to improve performance**\n",
    "\n",
    "Ensemble methods in boosting refer to combining several weak learners sequentially to create a stronger, more accurate model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43c01e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "840a2fd2",
   "metadata": {},
   "source": [
    "## Q10. In boosting, what does 'error minimization' involve? \n",
    "1. Reducing the size of the training set \n",
    "2. Adjusting the weights of misclassified samples to focus on difficult cases \n",
    "3. Increasing the number of features used for training \n",
    "4. Standardizing th einput features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a672dbe6",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Adjusting the weights of misclassified samples to focus on difficult cases**\n",
    "\n",
    "In boosting, error minimization involves giving more focus (higher weights) to the samples that were misclassified in previous rounds, so subsequent learners can better correct those errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33de7432",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d57d813",
   "metadata": {},
   "source": [
    "## Q11. What key concept distinguishes AdaBoost from other Boosting methods? \n",
    "1. It uses gradient descent to minimize errors \n",
    "2. It focuses on adjusting weights based on classification errors \n",
    "3. It applies regularization to prevent overfitting \n",
    "4. It perform feature selection before model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7428e61",
   "metadata": {},
   "source": [
    "The key concept that distinguishes AdaBoost is:\n",
    "\n",
    "**2. It focuses on adjusting weights based on classification errors**\n",
    "\n",
    "AdaBoost iteratively adjusts the weights of training samples, increasing weights for misclassified samples so that subsequent weak learners focus more on those hard cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eff479",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eeca470",
   "metadata": {},
   "source": [
    "## Q12. Which of the following is a common application of AdaBoost? \n",
    "1. Image classification \n",
    "2. Sentiment analysis \n",
    "3. Spam detection \n",
    "4. All of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec91b56",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**4. All of the above**\n",
    "\n",
    "AdaBoost is a versatile boosting algorithm used in various classification tasks including image classification, sentiment analysis, and spam detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e602c9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80448567",
   "metadata": {},
   "source": [
    "## Q13. How does Gradient Boosting improve model accuracy ? \n",
    "1. By combining multiple models in parallel \n",
    "2. By sequentially adding models to correct the errors of previous models \n",
    "3. By using a single model with complex architectures \n",
    "4. By applying unsupervised learning techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a84c5d6",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. By sequentially adding models to correct the errors of previous models**\n",
    "\n",
    "Gradient Boosting builds models one after another, where each new model tries to fix the mistakes made by the earlier ones, improving overall accuracy step by step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7b54b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bbad308",
   "metadata": {},
   "source": [
    "## Q14. What does the 'loss function' play in Gradient Boosting? \n",
    "1. It defines the criteria for model selection \n",
    "2. It measures how well the model's predictions match the actual outcomes \n",
    "3. It adjusts the learning rate of the model \n",
    "4. It performs feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6ae81c",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. It measures how well the model's predictions match the actual outcomes**\n",
    "\n",
    "In Gradient Boosting, the loss function quantifies the error between predicted and actual values, guiding the model to minimize this error during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128acbc3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dbc5e31",
   "metadata": {},
   "source": [
    "## Q15. Which algorithm is known for its speed and performance improvements in Gradient Boosting? \n",
    "1. AdaBoost \n",
    "2. XGBoost \n",
    "3. Random Forest \n",
    "4. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698346d",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. XGBoost**\n",
    "\n",
    "XGBoost is well-known for its speed and performance improvements over traditional Gradient Boosting implementations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7457086d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75b0fadd",
   "metadata": {},
   "source": [
    "## Q16. What is 'regularization' in the context of XGBoost? \n",
    "1. A technique to handle missing data \n",
    "2. A method to reduce overfitting by penalizing complex models \n",
    "3. A way to increase the size of the training data \n",
    "4. A procedure to normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eee9799",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. A method to reduce overfitting by penalizing complex models**\n",
    "\n",
    "In XGBoost, regularization techniques (like L1 and L2 penalties) help prevent the model from overfitting by discouraging overly complex trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553c603",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "053e4df4",
   "metadata": {},
   "source": [
    "## Q17. Which key concept in XGBoost helps in reducing overfitting? \n",
    "1. Boosting weak learners \n",
    "2. Tree pruning and regularization \n",
    "3. Weight adjustment in misclassified samples \n",
    "4. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa873a5",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Tree pruning and regularization**\n",
    "\n",
    "XGBoost reduces overfitting through techniques like tree pruning (removing unnecessary branches) and regularization (penalizing complexity), which help keep the model generalizable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a096e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdb27e69",
   "metadata": {},
   "source": [
    "## Q18. Which method is used to optimize performance in XGBoost? \n",
    "1. Gradient Descent \n",
    "2. Stochastic Gradient Descent \n",
    "3. Newton-Raphson method \n",
    "4. Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02cc6a3",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Newton-Raphson method**\n",
    "\n",
    "XGBoost uses a second-order Taylor expansion and the Newton-Raphson method for more efficient optimization during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb3a60a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f86d59e6",
   "metadata": {},
   "source": [
    "## Q19. What is the primary advantage of using Adaboost? \n",
    "1. It is less prone to overfitting compared to other boosting methods \n",
    "2. It handles large datasets more efficiently \n",
    "3. It combines multiple models to improve performance \n",
    "4. It provides robust results with weak classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e87f8",
   "metadata": {},
   "source": [
    "The best answer is:\n",
    "\n",
    "**4. It provides robust results with weak classifiers**\n",
    "\n",
    "AdaBoost effectively combines multiple weak classifiers to create a strong classifier, often yielding robust results even when individual learners are simple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f80269a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8714a814",
   "metadata": {},
   "source": [
    "## Q20. In Gradient Boosting, what does 'gradient descent' refer to?\n",
    "1. A technique to increase model complexity \n",
    "2. An optimization method to minimize the loss function \n",
    "3. A way to split the datset into training an dvalidation sets \n",
    "4. A process for festure selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f366a",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. An optimization method to minimize the loss function**\n",
    "\n",
    "In Gradient Boosting, gradient descent is used to iteratively minimize the loss by moving in the direction of the negative gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504eab4a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a013d77b",
   "metadata": {},
   "source": [
    "## Q21. What is a key limitation of AdaBoost? \n",
    "1. It cannot handle large datasets \n",
    "2. It is sensitive to noisy data and outliers \n",
    "3. It is computationally expensive \n",
    "4. It requires extensive feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d959f6",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. It is sensitive to noisy data and outliers**\n",
    "\n",
    "AdaBoost tends to focus heavily on misclassified points, which can cause it to overfit noisy data or outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf872b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5da0334",
   "metadata": {},
   "source": [
    "## Q22. Which boosting method is known for its efficiency in handling large datasets? \n",
    "1. AdaBoost \n",
    "2. Gradientt Boosting \n",
    "3. XGBoost \n",
    "4. Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543900ba",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. XGBoost**\n",
    "\n",
    "XGBoost is specifically designed for efficiency and scalability, making it well-suited for handling large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0852bca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "962b4767",
   "metadata": {},
   "source": [
    "## Q23. What does 'tree pruning' do in the context of XGBoost? \n",
    "1. It removes unnecessary features from the dataset \n",
    "2. It reduces the size of decision trees to prevent overfitting \n",
    "3. It adjusts the weights of misclassified samples \n",
    "4. It increases the depth of the trees to capture more complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449257b",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. It reduces the size of decision trees to prevent overfitting**\n",
    "\n",
    "Tree pruning in XGBoost removes branches that do not provide significant gain, helping to simplify the model and reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f9966f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "486385b5",
   "metadata": {},
   "source": [
    "## Q24. What is a common application of Gradient Boosting? \n",
    "1. Fraud detection \n",
    "2. Text classification \n",
    "3. Image segmentation \n",
    "4. All of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eac274",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**4. All of the above**\n",
    "\n",
    "Gradient Boosting is widely used across various domains including fraud detection, text classification, and image segmentation because of its strong predictive performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f022a1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89c8c6c4",
   "metadata": {},
   "source": [
    "## Q25. Which concept is not associated with boosting techniques? \n",
    "1. Weighted classification \n",
    "2. Gradient descent \n",
    "3. Feature selection \n",
    "4. Tree pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f9c85",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Feature selection**\n",
    "\n",
    "While boosting techniques involve weighted classification, gradient descent (in Gradient Boosting), and tree pruning (in methods like XGBoost), feature selection is not a core concept specific to boosting itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990eec70",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d94fbf6",
   "metadata": {},
   "source": [
    "## Q26. Which boosting method adjusts the weights of misclassified samples after each iteration? \n",
    "1. Gradient Boosting \n",
    "2. XGBoost \n",
    "3. AdaBoost \n",
    "4. Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd46295",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. AdaBoost**\n",
    "\n",
    "AdaBoost adjusts the weights of misclassified samples after each iteration to focus more on difficult cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6a5fa7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae8d8f87",
   "metadata": {},
   "source": [
    "## Q27. Which boosting technique uses a regularization term to control model complexity? \n",
    "1. AdaBoost \n",
    "2. Gradient Boosting \n",
    "3. XGBoost \n",
    "4. Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb0b49b",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. XGBoost**\n",
    "\n",
    "XGBoost includes regularization terms (L1 and L2) to control model complexity and prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ceddee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdebdaea",
   "metadata": {},
   "source": [
    "## Q28. In which scenario is AdaBoost least effective? \n",
    "1. When dealing with noisy datasets \n",
    "2. In applications requiring high accuracy \n",
    "3. For problems with a large number of features \n",
    "4. When the datset is well-balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53775a8",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**1. When dealing with noisy datasets**\n",
    "\n",
    "AdaBoost is sensitive to noise and outliers, so it tends to perform poorly when the dataset contains a lot of noisy data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c18dd5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81e063",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755095b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b518b81e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
