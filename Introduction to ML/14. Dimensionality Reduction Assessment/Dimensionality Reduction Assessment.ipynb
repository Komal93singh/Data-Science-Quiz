{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32509240",
   "metadata": {},
   "source": [
    "# Quiz: Dimensionality Reduction Assessment\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c9a3f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09d4ced8",
   "metadata": {},
   "source": [
    "## Q1. What is the key difference between PCA and LDA? \n",
    "1. PCA maximizes class separability , while LDA maximizes variance \n",
    "2. PCA is unspervised, while LDA is supervised \n",
    "3. PCA works only for binary classification, while LDA works for multi-class classification \n",
    "4. PCA is a non-linear method, while LDA is linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb7d3cd",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. PCA is unsupervised, while LDA is supervised**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **PCA (Principal Component Analysis)** is an **unsupervised** dimensionality reduction technique that finds directions (principal components) that maximize the variance in the data, without using class labels.\n",
    "\n",
    "* **LDA (Linear Discriminant Analysis)** is a **supervised** technique that finds the linear combinations of features that best separate the classes by maximizing the ratio of between-class variance to within-class variance.\n",
    "\n",
    "Other options:\n",
    "\n",
    "1. Wrong — PCA maximizes variance, LDA maximizes class separability.\n",
    "2. Wrong — PCA is not classification-based; LDA works for multi-class classification as well as binary.\n",
    "3. Wrong — Both PCA and LDA are linear methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79ab3e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ead5412",
   "metadata": {},
   "source": [
    "## Q2. What happens to the separability of classes in LDA if the within-class scatter matrix is singular or nearly singular? \n",
    "1. The separability is maximized \n",
    "2. The LDA solution is not unique, leading to poor class separability \n",
    "3. The separability is unaffected \n",
    "4. The classes become linearly inseparable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de0095f",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. The LDA solution is not unique, leading to poor class separability**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* LDA involves calculating the **within-class scatter matrix (Sw)** and the **between-class scatter matrix (Sb)**.\n",
    "* To find the optimal projection, LDA solves the generalized eigenvalue problem involving the inverse of the within-class scatter matrix: ( S_w^{-1} S_b ).\n",
    "* If **Sw is singular or nearly singular** (i.e., not invertible or ill-conditioned), the solution becomes unstable or **not unique**.\n",
    "* This instability means the projection directions are not well-defined, which can degrade the ability to separate classes effectively.\n",
    "\n",
    "Other options:\n",
    "\n",
    "1. Incorrect — Singular Sw does not maximize separability.\n",
    "2. Incorrect — Separability is affected by singular Sw.\n",
    "3. Incorrect — Classes might still be linearly separable but LDA can’t find the solution properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bba02c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6932087f",
   "metadata": {},
   "source": [
    "## Q3. Consider a dataset with 10,000 features. After applying PCA, how would you determine the optimal number of principal components to retain? \n",
    "1. Retain components that account for at least 90% of the total variance \n",
    "2. Retain the first 100 Principal components \n",
    "3. Retain components based on their eigenvalues being greater than one \n",
    "4. Retain components until the explained variance begins to decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82290ad3",
   "metadata": {},
   "source": [
    "The best answer is:\n",
    "\n",
    "**1. Retain components that account for at least 90% of the total variance**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* When applying PCA, a common strategy to choose the number of principal components is to select enough components to explain a large percentage (commonly 90% or 95%) of the total variance in the data.\n",
    "* This balances dimensionality reduction with retaining most of the important information.\n",
    "\n",
    "Other options:\n",
    "\n",
    "2. Arbitrary fixed number (100) may not suit every dataset and may retain too many or too few components.\n",
    "3. The \"eigenvalue greater than one\" rule is commonly used in **Factor Analysis** or **Kaiser criterion**, but less standard for PCA with very high dimensions.\n",
    "4. Explained variance does not typically “begin to decrease” as you add more components — it always increases or stays constant because components are ordered by decreasing variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d9902",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14024dac",
   "metadata": {},
   "source": [
    "## Q4. Which of the following statements best describes the relationship between the Curse of Dimensionality and the intrinsic dimensionality of data? \n",
    "1. The Curse of Dimensionality only affects data with low intrinsic dimensionality \n",
    "2. The Curse of Dimensionality arises when the number of features exceeds the intrinsic dimensionality of the data \n",
    "3. The intrinsic dimensionality of data increases as the number of features increases \n",
    "4. The Curse of Dimensionality does not depend on intrinsic dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf9524",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. The Curse of Dimensionality arises when the number of features exceeds the intrinsic dimensionality of the data**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Intrinsic dimensionality** refers to the minimum number of variables needed to represent the data without significant information loss.\n",
    "* The **Curse of Dimensionality** occurs when the **ambient dimensionality** (actual number of features) is much higher than the intrinsic dimensionality, causing problems like data sparsity, overfitting, and difficulty in distance-based measures.\n",
    "* When the number of features exceeds the true intrinsic dimensionality, the data becomes sparse in the high-dimensional space, which negatively impacts many algorithms.\n",
    "\n",
    "Other options:\n",
    "\n",
    "1. Incorrect — Curse affects data when high dimensionality exceeds intrinsic dimensionality, not only low intrinsic dimensionality data.\n",
    "2. Incorrect — Intrinsic dimensionality is a property of the data structure, not directly increased by adding irrelevant features.\n",
    "3. Incorrect — Curse of Dimensionality is closely related to intrinsic dimensionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd3743",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d10287f",
   "metadata": {},
   "source": [
    "## Q5. Which of th efollowing statements is true about PCA and LDA? \n",
    "1. Both PCA and LDA require labelled data for training \n",
    "2. PCA maximizes variance, while LDA maximizes class separbility \n",
    "3. LDA is always superior to PCA for all datasets \n",
    "4. PCA is a supervised technique, while LDA is unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15753c",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. PCA maximizes variance, while LDA maximizes class separability**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **PCA** is an **unsupervised** technique that finds directions (principal components) maximizing the overall variance in the data, without using class labels.\n",
    "* **LDA** is a **supervised** technique that aims to find a projection maximizing the separability between different classes.\n",
    "\n",
    "Other options:\n",
    "\n",
    "1. Incorrect — PCA does **not** require labeled data, but LDA does.\n",
    "2. Incorrect — LDA is not always superior; it depends on the dataset and task.\n",
    "3. Incorrect — PCA is unsupervised, LDA is supervised.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9546c16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df91b737",
   "metadata": {},
   "source": [
    "## Q6. What is the primary purpose of applying dimensionality reduction before using a machine learning model? \n",
    "1. To improve the performance of distance-based algorithms \n",
    "2. To increase the number of features in th edataset \n",
    "3. To remobve noise from the dataset \n",
    "4. To make the datset easier to visalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffd997f",
   "metadata": {},
   "source": [
    "The best answer is:\n",
    "\n",
    "**1. To improve the performance of distance-based algorithms**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Dimensionality reduction helps **reduce the number of features**, which often improves the performance of machine learning models, especially **distance-based algorithms** like k-NN or clustering, because these algorithms suffer from the Curse of Dimensionality.\n",
    "* By reducing dimensions, data becomes less sparse, distances become more meaningful, and models often perform better.\n",
    "\n",
    "Other options:\n",
    "\n",
    "2. Incorrect — Dimensionality reduction **decreases** the number of features, not increases.\n",
    "3. Partially true — Dimensionality reduction can help reduce noise indirectly, but it's not the **primary** purpose.\n",
    "4. Also true — Dimensionality reduction aids visualization (e.g., reducing to 2D or 3D), but this is more of a side benefit than the main purpose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0eff9f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f67d429",
   "metadata": {},
   "source": [
    "## Q7. Which technique would you use to visualize the separation between different classes in a dataset? \n",
    "1. PCA \n",
    "2. LDA \n",
    "3. t-SNE \n",
    "4. Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f51a4b",
   "metadata": {},
   "source": [
    "The best answer is:\n",
    "\n",
    "**3. t-SNE**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **t-SNE (t-Distributed Stochastic Neighbor Embedding)** is specifically designed for visualizing high-dimensional data by preserving local structures and revealing clusters, making it excellent for visualizing separation between different classes.\n",
    "* While **PCA** and **LDA** can also be used for visualization:\n",
    "\n",
    "  * **PCA** focuses on variance, not class separation.\n",
    "  * **LDA** is supervised and tries to maximize class separability but may not capture complex structures.\n",
    "* **Autoencoders** are primarily for nonlinear dimensionality reduction or feature learning, not primarily visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029634c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c672f5d",
   "metadata": {},
   "source": [
    "## Q8. When applying PCA, why is it important to standardize the data? \n",
    "1. To ensure that all variables have equal  variance \n",
    "2. To reduce th enumber of principal components \n",
    "3. To maximize the explained variance \n",
    "4. To increase the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ecf74",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**1. To ensure that all variables have equal variance**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* PCA is sensitive to the scale of the features because it relies on the covariance matrix.\n",
    "* If features have different units or scales, those with larger variance will dominate the principal components.\n",
    "* **Standardizing** (usually mean=0, variance=1) puts all features on the same scale, so PCA treats each feature equally.\n",
    "\n",
    "Other options:\n",
    "\n",
    "2. Incorrect — Standardization doesn’t reduce the number of components directly.\n",
    "3. Incorrect — Standardization doesn’t maximize explained variance; it balances variance across features.\n",
    "4. Incorrect — Standardization does not increase the number of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ade14e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cab8a4e",
   "metadata": {},
   "source": [
    "## Q9. Which of the following is a disadvantage of LDA? \n",
    "1. It requires more computation compared to PCA \n",
    "2. It works only for binary classification problems \n",
    "3. It assumes that the data is normally distributed within each class \n",
    "4. It cannot be used with high-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d9f1f4",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. It assumes that the data is normally distributed within each class**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* LDA assumes that the features for each class follow a **multivariate normal (Gaussian) distribution** with the same covariance matrix across classes.\n",
    "* This assumption can be violated in real-world data, reducing LDA’s effectiveness.\n",
    "\n",
    "Other options:\n",
    "\n",
    "1. Incorrect — LDA is generally less computationally expensive than some other methods; PCA often requires eigen decomposition but both are comparable.\n",
    "2. Incorrect — LDA can handle multi-class classification, not just binary.\n",
    "3. Incorrect — LDA can be used with high-dimensional data, though it might face issues like singular covariance matrices if the dimensionality is too high relative to the sample size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def5e78",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1a8a5b7",
   "metadata": {},
   "source": [
    "## Q10. What is a common use of PCA in image processing? \n",
    "1. Image segmentation \n",
    "2. Image compression \n",
    "3. Edge detection \n",
    "4. Color correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea5e62",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Image compression**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* PCA reduces the dimensionality of image data by projecting it onto principal components that capture the most variance.\n",
    "* This allows images to be stored or transmitted using fewer components while preserving important information, effectively compressing the image.\n",
    "\n",
    "Other options:\n",
    "\n",
    "1. Image segmentation — usually done using clustering or other methods, not PCA.\n",
    "2. Edge detection — relies on filters like Sobel or Canny, not PCA.\n",
    "3. Color correction — involves color space transformations, not PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3735edb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da410d9a",
   "metadata": {},
   "source": [
    "## Q11. In PCA, what do eigenvectors represent? \n",
    "1. The amount of variance explained by each principal component \n",
    "2. The directions of maximum variance in the data \n",
    "3. The correlation between variables \n",
    "4. The covariance between principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53391595",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. The directions of maximum variance in the data**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* In PCA, **eigenvectors** represent the directions (principal components) along which the data varies the most.\n",
    "* Each eigenvector defines a new axis in the transformed feature space.\n",
    "* The corresponding **eigenvalues** indicate how much variance is explained by each eigenvector (principal component).\n",
    "\n",
    "Other options:\n",
    "\n",
    "1. Incorrect — That describes eigenvalues, not eigenvectors.\n",
    "2. Incorrect — Correlation is between variables, not what eigenvectors represent.\n",
    "3. Incorrect — Covariance between principal components is zero (they are orthogonal).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ed7235",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "146c82e4",
   "metadata": {},
   "source": [
    "## Q12. What is the primary limitation of PCA? \n",
    "1. It can  only be applied to categorical data \n",
    "2. It assumes linear relationships between variables \n",
    "3. It always increases model complexity \n",
    "4. It cannot be sued for data visalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ec131",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. It assumes linear relationships between variables**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* PCA is a **linear** dimensionality reduction technique that projects data onto linear combinations of original features.\n",
    "* It cannot capture **non-linear** relationships between variables effectively.\n",
    "\n",
    "Other options:\n",
    "\n",
    "1. Incorrect — PCA works with continuous numeric data, not categorical data directly.\n",
    "2. Incorrect — PCA generally reduces dimensionality and often decreases model complexity.\n",
    "3. Incorrect — PCA is commonly used for data visualization (e.g., 2D or 3D plots).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb31cd63",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65077482",
   "metadata": {},
   "source": [
    "## Q13. Which of the following scenarios is LDA particularly useful for? \n",
    "1. Reducing dimensionality in a regression problem \n",
    "2. Reducing dimensionality in a multi-class classification problem \n",
    "3. Visualizing data in 3D space \n",
    "4. Handling missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e042762",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Reducing dimensionality in a multi-class classification problem**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* LDA is a **supervised** dimensionality reduction technique designed to maximize class separability.\n",
    "* It works well in multi-class classification problems by projecting data onto a lower-dimensional space while preserving class discriminability.\n",
    "\n",
    "Other options:\n",
    "\n",
    "1. Incorrect — LDA is not used for regression problems.\n",
    "2. While LDA can reduce dimensions to 3D, its main purpose is class separation, not just visualization.\n",
    "3. Incorrect — LDA does not inherently handle missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0d3e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e02af7e",
   "metadata": {},
   "source": [
    "## Q14. What is the \"Curse of Dimensionality\"? \n",
    "1. The phenomenon where the amount of data needed to generalize accurately increases exponentially with the number of features \n",
    "2. The process of reducing the number of dimensions in a dataset \n",
    "3. The problem of having too few data points in a dataset \n",
    "4. The situation where data becomes easier to visualize with more dimesions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee21944",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**1. The phenomenon where the amount of data needed to generalize accurately increases exponentially with the number of features**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* The **Curse of Dimensionality** refers to various problems that arise when dealing with high-dimensional data.\n",
    "* One key aspect is that as the number of features (dimensions) increases, the volume of the feature space grows exponentially, making data points sparse.\n",
    "* This sparsity means that you need exponentially more data to reliably learn patterns and generalize well.\n",
    "\n",
    "Other options:\n",
    "\n",
    "2. Incorrect — That describes dimensionality reduction, not the curse.\n",
    "3. Incorrect — Having too few data points is related but not the definition of the curse.\n",
    "4. Incorrect — More dimensions generally make visualization harder, not easier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bcad44",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cccb0f4",
   "metadata": {},
   "source": [
    "## Q15. Which of the following best describes LDA (Linear Discriminant Analysis)? \n",
    "1. A method for clustering data into groups \n",
    "2. A dimensionality reduction technique that also performs classification \n",
    "3. A technique used solely for feature selection\n",
    "4. A non-linear dimensionality reduction technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880131e",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. A dimensionality reduction technique that also performs classification**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* LDA is primarily a **supervised dimensionality reduction** method that projects data onto a lower-dimensional space to maximize class separability.\n",
    "* It can also be used as a **classifier** by assigning new samples to classes based on the learned discriminant functions.\n",
    "\n",
    "Other options:\n",
    "\n",
    "1. Incorrect — Clustering is unsupervised, whereas LDA is supervised.\n",
    "2. Incorrect — LDA reduces dimensionality but is not just for feature selection.\n",
    "3. Incorrect — LDA is a **linear** technique, not non-linear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd5524",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b36d4db0",
   "metadata": {},
   "source": [
    "## Q16. What is the main advantage of using PCA for dimensionality reduction? \n",
    "1. It always increases model accuracy \n",
    "2. It reduces the number of dimensions while retaining most of the data's variance \n",
    "3. It simplifies data without any loss of information \n",
    "4. It increases the interpretability of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a340676",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. It reduces the number of dimensions while retaining most of the data's variance**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* PCA transforms the data into a smaller set of uncorrelated variables (principal components) that capture most of the original variance.\n",
    "* This helps simplify the dataset while preserving the most important information.\n",
    "\n",
    "Other options:\n",
    "\n",
    "1. Incorrect — PCA doesn’t always increase accuracy; it depends on the model and data.\n",
    "2. Incorrect — PCA involves some loss of information because it reduces dimensionality.\n",
    "3. Incorrect — PCA components are linear combinations and often less interpretable than original features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abead97",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a48f022f",
   "metadata": {},
   "source": [
    "## Q17. Which of the following is NOT a type of dimensionality reduction technique? \n",
    "1. Feature Selection \n",
    "2. Feature Extraction \n",
    "3. Feature Engineering \n",
    "4. Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472650c5",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Feature Engineering**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Dimensionality reduction** involves reducing the number of features either by selecting a subset (**Feature Selection**, **Feature Elimination**) or by transforming features (**Feature Extraction**).\n",
    "* **Feature Engineering** is a broader process of creating new features or modifying existing ones but is not specifically a dimensionality reduction technique.\n",
    "\n",
    "Other options are types of dimensionality reduction methods:\n",
    "\n",
    "* **Feature Selection:** Choosing a subset of original features.\n",
    "* **Feature Extraction:** Creating new features from original features (e.g., PCA).\n",
    "* **Feature Elimination:** Removing irrelevant or redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c48c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae14043e",
   "metadata": {},
   "source": [
    "## Q18. In PCA, what does the first principal component represent? \n",
    "1. The component with the least variance \n",
    "2. The direction with the most variance in the dataset \n",
    "3. The direction with the least correlation with the dataset \n",
    "4. The direction of the smallest eigenvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6c8c2e",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. The direction with the most variance in the dataset**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* The **first principal component** is the linear combination of original features that captures the **maximum variance** in the data.\n",
    "* It represents the direction along which the data varies the most.\n",
    "\n",
    "Other options:\n",
    "\n",
    "1. Incorrect — It represents the *most* variance, not the least.\n",
    "2. Incorrect — It does not necessarily relate to correlation in that way.\n",
    "3. Incorrect — It corresponds to the largest eigenvalue, not the smallest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce41b70",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f327df4",
   "metadata": {},
   "source": [
    "## Q19. Which technique is a linear method for dimensionality reduction? \n",
    "1. PCA (Principal Component Analysis) \n",
    "2. t-SNE ( t-Distributed Stochastic Neighbor Embedding) \n",
    "3. UMAP (Uniform Manifold Approximation and Projection) \n",
    "4. Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ffeee4",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**1. PCA (Principal Component Analysis)**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **PCA** is a **linear** dimensionality reduction technique that projects data onto orthogonal directions of maximum variance.\n",
    "* The others (**t-SNE, UMAP, Autoencoder**) are **non-linear** methods designed to capture complex structures in data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159743de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f7990d6",
   "metadata": {},
   "source": [
    "## Q20. What is the primary goal of dimensionality reduction? \n",
    "1. To increase the number of features in a dataset \n",
    "2. To retain the most important information in data while reducing the number of features \n",
    "3. To remove outliers from the dataset \n",
    "4. To eliminate noise from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d81379c",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. To retain the most important information in data while reducing the number of features**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* The main goal of dimensionality reduction is to simplify data by reducing its features, while preserving the key information or structure.\n",
    "* This helps improve model performance, reduce computation, and sometimes aids visualization.\n",
    "\n",
    "Other options:\n",
    "\n",
    "1. Incorrect — Dimensionality reduction reduces features, not increase them.\n",
    "   3 & 4. Incorrect — While noise reduction or outlier handling might be side effects, they are not the primary goal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0434e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98b8c56b",
   "metadata": {},
   "source": [
    "## Q21. Which of the following is NOT an effect of high dimensionality? \n",
    "1. Data sparsity \n",
    "2. Increased computation \n",
    "3. Reduced model complexity \n",
    "4. Performance degradation of distance-based algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b7f66",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Reduced model complexity**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* High dimensionality usually **increases** model complexity because there are more features to consider.\n",
    "* Effects of high dimensionality include:\n",
    "\n",
    "  * **Data sparsity** — points become spread out in space.\n",
    "  * **Increased computation** — more features mean more calculations.\n",
    "  * **Performance degradation of distance-based algorithms** — distances become less meaningful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ba191f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58e55781",
   "metadata": {},
   "source": [
    "## Q22. In the context of LDA, what does the term \"discriminant\" refer to? \n",
    "1. A feature that is discarded during dimensionality reduction \n",
    "2. A linear combination of features that separates classes \n",
    "3. A type of noise in the dataset \n",
    "4. A technique used for clustering data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bdd4e2",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. A linear combination of features that separates classes**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* In LDA, a **discriminant** is a linear combination of the original features that best separates different classes.\n",
    "* These discriminants form the new axes in the reduced-dimensional space to maximize class separability.\n",
    "\n",
    "Other options:\n",
    "\n",
    "1. Incorrect — Discriminants are not discarded features.\n",
    "2. Incorrect — It's not noise.\n",
    "3. Incorrect — LDA is supervised, not a clustering technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d379a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4729983d",
   "metadata": {},
   "source": [
    "## Q23. When performing PCA on a dataset, why might you choose to perform a whitening transformation on the principal components? \n",
    "1. To increase the dimensionality of the data \n",
    "2. To make the components uncorrelated with unit variance \n",
    "3. To reduce the noise in the data \n",
    "4. To enforce a particular order in the components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c78b5",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. To make the components uncorrelated with unit variance**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Whitening** transforms the principal components so that they are **uncorrelated** and each has **unit variance**.\n",
    "* This standardizes the scale of components, which can be useful for certain machine learning algorithms that assume features have similar scales.\n",
    "\n",
    "Other options:\n",
    "\n",
    "1. Incorrect — Whitening does not increase dimensionality.\n",
    "2. Incorrect — Whitening is not primarily for noise reduction.\n",
    "3. Incorrect — The order of components is based on explained variance, not whitening.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c46f51",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "782dd697",
   "metadata": {},
   "source": [
    "## Q24. In PCA, if the data is highly correlated, what can be expected regarding the number of principal components requires to capture most of the variance? \n",
    "1. A large number of principal components will be required \n",
    "2. Only one principal component will be sufficient \n",
    "3. A small number of principal components will capture most of the variance \n",
    "4. The correlation among features has no impact on th enumber of principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06801162",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. A small number of principal components will capture most of the variance**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* When features are highly correlated, much of the variance lies along fewer directions.\n",
    "* Therefore, a **small number** of principal components can capture most of the variance.\n",
    "* This is why PCA is effective for reducing dimensionality in correlated datasets.\n",
    "\n",
    "Other options:\n",
    "\n",
    "1. Incorrect — Highly correlated data usually requires fewer components, not more.\n",
    "2. Incorrect — Usually more than one component is needed unless data is perfectly correlated.\n",
    "3. Incorrect — Correlation directly affects how many components explain variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd61db24",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75d03afb",
   "metadata": {},
   "source": [
    "## Q25. In LDA, how does the dimensionality of the projected space relate to the number of classes ccc in the dataset? \n",
    "1. The dimensionality of the projected space is equal to the number of classes ccc \n",
    "2. The dimensionality of the projected space is c-1c - 1c -1 \n",
    "3. The dimensionality of the projected space is c+1c + 1c+1 \n",
    "4. The dimensionality of the projected space is always less than c-1c - 1c-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609d0370",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. The dimensionality of the projected space is ( c - 1 )**\n",
    "* In LDA, the dimensionality of the space onto which the data is projected is c−1c - 1c−1, where ccc is the number of classes.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* In LDA, if you have **( c )** classes, the maximum number of discriminant vectors (dimensions) you can get is **( c - 1 )**.\n",
    "* This is because LDA finds directions that best separate the classes, and the number of such directions is limited by the number of classes minus one.\n",
    "\n",
    "Other options:\n",
    "\n",
    "1. Incorrect — It’s not equal to the number of classes, but one less.\n",
    "2. Incorrect — It’s not ( c + 1 ).\n",
    "3. Incorrect — The dimensionality is exactly ( c - 1 ), not always less.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de403ad4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89df3118",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12517855",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1211cd7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
