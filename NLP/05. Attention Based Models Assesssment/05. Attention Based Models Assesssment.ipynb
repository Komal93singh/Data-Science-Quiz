{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a7dd892",
   "metadata": {},
   "source": [
    "# Quiz : Attention Based Models Assesssment\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9cb52",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51a68edd",
   "metadata": {},
   "source": [
    "### Q1. What is the primary purpose of a sequence-to-sequence (seq2seq) model? \n",
    "1. Classification \n",
    "2. Sequence generation \n",
    "3. Image recognition \n",
    "4. Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012133d0",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Sequence generation**\n",
    "\n",
    "Seq2seq models are primarily used for tasks where one sequence needs to be transformed into another, such as **machine translation, text summarization, and speech recognition**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33605c37",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f95a0499",
   "metadata": {},
   "source": [
    "### Q2. Which components are essential in a Seq2Seq model? \n",
    "1. Input layer and output layer \n",
    "2. Encoder and decoder \n",
    "3. Convolutional layers \n",
    "4. Dropout and batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f1409d",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Encoder and decoder** \n",
    "\n",
    "A **Seq2Seq model** consists of:\n",
    "\n",
    "* **Encoder** → processes the input sequence and compresses it into a context (hidden state).\n",
    "* **Decoder** → generates the output sequence step by step using that context.\n",
    "\n",
    "Other options (like convolutional layers, dropout, etc.) can be used additionally, but **encoder–decoder is the core architecture**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea7fcf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69f89eba",
   "metadata": {},
   "source": [
    "### Q3. In a Seq2Seq model, what does the encoder output? \n",
    "1. A sequence of words \n",
    "2. A fixed-size context vector \n",
    "3. A classification label \n",
    "4. An attention score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da406dc",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. A fixed-size context vector** \n",
    "\n",
    "In a standard **Seq2Seq model**, the **encoder** processes the input sequence and compresses it into a **context vector (hidden state)**. This vector captures the essential information of the input sequence and is passed to the **decoder**, which generates the output sequence.\n",
    "\n",
    "With **attention mechanisms**, instead of just a single fixed-size vector, the encoder can output a **sequence of hidden states**, and the decoder selectively focuses on different parts — but in the basic Seq2Seq, it's a **fixed-size context vector**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf563b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64224033",
   "metadata": {},
   "source": [
    "### Q4. What mechanism allows a model to focus on specific parts of an input sequence? \n",
    "1. Dropout \n",
    "2. Pooling \n",
    "3. Attention \n",
    "4. Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d49b6",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Attention**\n",
    "\n",
    "The **attention mechanism** allows a model to focus on the most relevant parts of the input sequence at each step of decoding, rather than relying only on a single fixed-size context vector.\n",
    "\n",
    "This is especially useful in tasks like **machine translation**, where different words in the input sequence may be more important at different stages of generating the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c011446",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9094857",
   "metadata": {},
   "source": [
    "### Q5. Which attention mechanism is also known as dot-product attention? \n",
    "1. Additive attention \n",
    "2. Multiplicative attention \n",
    "3. Self-attention \n",
    "4. Contextual attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c420b",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Multiplicative attention**\n",
    "\n",
    "* **Additive attention** → uses a feedforward network with a learned weight to compute alignment scores.\n",
    "* **Multiplicative attention (dot-product attention)** → computes alignment scores by taking the **dot product** between the encoder hidden state and the decoder hidden state.\n",
    "* **Self-attention** → applies attention within the same sequence (used in Transformers).\n",
    "\n",
    "So, **dot-product attention = multiplicative attention**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4f061",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3d8337e",
   "metadata": {},
   "source": [
    "### Q6. What is the key benefit of using attention mechanisms in Seq2Seq models? \n",
    "1. Reducing model complexity \n",
    "2. Improving long-range dependency capture \n",
    "3. Enhancing model regularization \n",
    "4. Accelerating model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89afea3d",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Improving long-range dependency capture**\n",
    "\n",
    "Attention mechanisms allow the decoder to **directly access relevant parts of the input sequence** instead of relying only on a single fixed-size context vector. This helps the model better handle **long sequences** and capture **long-range dependencies**, which is a major limitation of basic Seq2Seq models without attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe56ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d1d7ec8",
   "metadata": {},
   "source": [
    "### Q7. What does self-attention compute? \n",
    "1. Attention scores between different sequences \n",
    "2. Attention scores between different positions in the same sequence \n",
    "3. Loss function \n",
    "4. Gradient descent steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940b2990",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Attention scores between different positions in the same sequence**\n",
    "\n",
    "**Self-attention** (used in Transformers) lets each token in a sequence look at and weigh the importance of **all other tokens in that same sequence**.\n",
    "\n",
    "* Example: In the sentence *“The cat sat on the mat”*, the word **“cat”** can attend to **“the”** and **“mat”** to understand its context.\n",
    "\n",
    "This is what makes Transformers powerful for capturing **contextual relationships** within sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721d0ee1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29956aab",
   "metadata": {},
   "source": [
    "### Q8. Which model architecture first introduced the self-attention mechanism? \n",
    "1. RNN \n",
    "2. LSTM \n",
    "3. Transformer \n",
    "4. GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28487609",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Transformer**\n",
    "\n",
    "The **Transformer architecture** (introduced in the paper *“Attention Is All You Need”*, 2017) was the first to fully rely on **self-attention** mechanisms, completely removing recurrence (RNNs, LSTMs, GRUs).\n",
    "\n",
    "This innovation made Transformers highly effective for **parallelization, long-range dependency handling, and large-scale training**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9a78ed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abf0f65f",
   "metadata": {},
   "source": [
    "### Q9. What is the primary function of the decoder in a Seq2Seq model? \n",
    "1. To classify input data \n",
    "2. To generate an output sequence from a context vector \n",
    "3. To reduce dimensionality \n",
    "4. To preprocess input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd2bd8",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. To generate an output sequence from a context vector**\n",
    "\n",
    "In a **Seq2Seq model**:\n",
    "\n",
    "* The **encoder** processes the input sequence and encodes it into a **context vector (or hidden states with attention)**.\n",
    "* The **decoder** takes this context and **generates the output sequence step by step**, often predicting the next token based on the context and previously generated tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363d94d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18df02f5",
   "metadata": {},
   "source": [
    "### Q10. In Transformer models, what is the purpose of positional encoding? \n",
    "1. To add non-linearity \n",
    "2. To encode the position of tokens in the sequence \n",
    "3. To increase the model's depth \n",
    "4. To perform data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bace06",
   "metadata": {},
   "source": [
    "\n",
    "The correct answer is:\n",
    "\n",
    "**2. To encode the position of tokens in the sequence**\n",
    "\n",
    "Since **Transformers** rely only on **self-attention** (which is order-invariant), they have no inherent sense of word order.\n",
    "**Positional encoding** injects information about the **position of each token** in the sequence (e.g., first word, second word, etc.), allowing the model to understand sequence structure.\n",
    "\n",
    "Without positional encoding, a Transformer would treat a sentence like *“dog bites man”* the same as *“man bites dog”*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a5760",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "593de589",
   "metadata": {},
   "source": [
    "### Q11. Which of the following is a characteristic feature of the LSTM architecture? \n",
    "1. Linear activation function \n",
    "2. Gating mechanisms \n",
    "3. Dropout layers \n",
    "4. Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577cdacc",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Gating mechanisms**\n",
    "\n",
    "LSTMs (Long Short-Term Memory networks) use **input, forget, and output gates** to regulate the flow of information.\n",
    "\n",
    "* These gates help the model **retain long-term dependencies** and reduce the **vanishing gradient problem** common in standard RNNs.\n",
    "\n",
    "Other options (like dropout or positional encoding) can be added, but they are **not defining features** of LSTMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f4536e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2ee490f",
   "metadata": {},
   "source": [
    "### Q12. How does the forget gate in an LSTM cell contribute to its function? \n",
    "1. It decides which parts of the current input to ignore \n",
    "2. It decides which parts of the cell state to erase \n",
    "3. It normalizes the input sequence \n",
    "4. It adds noise to the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae0ad6",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. It decides which parts of the cell state to erase**\n",
    "\n",
    "In an **LSTM cell**:\n",
    "\n",
    "* The **forget gate** controls which information from the **previous cell state** should be **discarded (erased)** and which should be kept.\n",
    "* This allows the LSTM to **remove irrelevant information** and retain only useful long-term dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ad408",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "296b81c5",
   "metadata": {},
   "source": [
    "### Q13. What is the main advantage of using a Bidirectional LSTM (Bi-LSTM)? \n",
    "1. Faster training speed \n",
    "2. Capturing information from both past and future contexts \n",
    "3. Reducing overfitting \n",
    "4. Increasing model regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef3b473",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Capturing information from both past and future contexts**\n",
    "\n",
    "A **Bidirectional LSTM (Bi-LSTM)** processes the sequence in **both directions**:\n",
    "\n",
    "* Forward LSTM → captures **past context** (left to right).\n",
    "* Backward LSTM → captures **future context** (right to left).\n",
    "\n",
    "This is especially useful in tasks like **speech recognition, text classification, and machine translation**, where context from both directions improves understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa47884",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17574049",
   "metadata": {},
   "source": [
    "### Q14. Which of the following is NOT a type of attention mechanism? \n",
    "1. Additive attention \n",
    "2. Multiplicative attention \n",
    "3. Recursive attention \n",
    "4. Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649fa73a",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Recursive attention**\n",
    "\n",
    "* **Additive attention** → Uses a feedforward network to compute alignment scores.\n",
    "* **Multiplicative (dot-product) attention** → Uses dot products to compute similarity scores.\n",
    "* **Self-attention** → Computes attention within the same sequence (core of Transformers).\n",
    "\n",
    "**Recursive attention** is **not a standard attention mechanism** in Seq2Seq or Transformer models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58d96c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1b7e81e",
   "metadata": {},
   "source": [
    "### Q15. How does the update gate in a GRU differ from an LSTM's input gate? \n",
    "1. It does not exist \n",
    "2. It combines the functionality of the forget and input gates \n",
    "3. It only controls the cell state \n",
    "4. It reduces overfitting in GRU models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7671c4fc",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. It combines the functionality of the forget and input gates**\n",
    "\n",
    "* In **LSTMs**, we have **separate forget and input gates**:\n",
    "\n",
    "  * Forget gate → decides what to erase from the cell state.\n",
    "  * Input gate → decides what new information to add.\n",
    "\n",
    "* In **GRUs**, the **update gate** merges these two roles into one:\n",
    "\n",
    "  * It controls both how much of the **past information** to keep and how much of the **new input** to add.\n",
    "\n",
    "This makes **GRUs simpler** (fewer parameters) compared to LSTMs, while still capturing dependencies effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48124cf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bac4ec84",
   "metadata": {},
   "source": [
    "### Q16. What role do residual connections play in Transformer models? \n",
    "1. They add noise to the model to prevent overfitting \n",
    "2. They improve gradient flow and model stability \n",
    "3. They reduce the model's complexity \n",
    "4. They increase the attention scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3275c",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. They improve gradient flow and model stability**\n",
    "\n",
    "In **Transformer models**, **residual connections (skip connections)**:\n",
    "\n",
    "* Allow gradients to flow more easily through deep networks → reducing the **vanishing gradient problem**.\n",
    "* Help preserve the original input signal by adding it back after each sub-layer (attention or feedforward).\n",
    "* Improve **training stability** and enable stacking of many layers without performance degradation.\n",
    "\n",
    "That’s why residual connections are a **key ingredient** in deep architectures like Transformers, ResNets, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99344140",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82a549f8",
   "metadata": {},
   "source": [
    "### Q17. Which of the following statements about beam search is correct? \n",
    "1. It is used to train Seq2Seq models \n",
    "2. It is a decoding algorithm that considers multiple sequences \n",
    "3. It accelerates the training process \n",
    "4. It reduces the number of parameters in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116490d9",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. It is a decoding algorithm that considers multiple sequences**\n",
    "\n",
    "* **Beam search** is used during **decoding** (not training) in Seq2Seq and Transformer models.\n",
    "* Instead of picking only the single most probable token at each step (like greedy search), it keeps track of the **top-k most likely sequences** (the “beam width”).\n",
    "* This helps generate more accurate and meaningful sequences in tasks like **machine translation, text summarization, and speech recognition**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135693ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf1763c3",
   "metadata": {},
   "source": [
    "### Q18. What is the main reason for scaling the dot-product in Scaled Dot-product Attention? \n",
    "1. To normalize the attention weights \n",
    "2. To prevent the softmax function from producing extremely small gradients \n",
    "3. To increase the model's capacity \n",
    "4. To add non-linearity to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3f77a",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. To prevent the softmax function from producing extremely small gradients**\n",
    "\n",
    "In **Scaled Dot-Product Attention**:\n",
    "\n",
    "* The dot product between **query (Q)** and **key (K)** can grow large when the vector dimension (**dₖ**) is high.\n",
    "* Large values push the **softmax** into regions where it outputs very small gradients → making training unstable.\n",
    "* To fix this, the dot product is scaled by **1 / √dₖ**, which keeps values in a reasonable range and stabilizes learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d79939f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73ebe73c",
   "metadata": {},
   "source": [
    "### Q19. In a Transformer, what does the term \"multi-head attention\" refer to? \n",
    "1. Using multiple sequence for attention \n",
    "2. Parallel attention mechanisms with different parameters \n",
    "3. Attention applied to multiple layers \n",
    "4. Attention applied across different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e542ec00",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Parallel attention mechanisms with different parameters** \n",
    "\n",
    "In **multi-head attention**:\n",
    "\n",
    "* The input is projected into multiple **query, key, and value** spaces (heads).\n",
    "* Each head learns to focus on **different aspects of the sequence** (e.g., short-range vs. long-range dependencies).\n",
    "* The outputs of all heads are concatenated and combined.\n",
    "\n",
    "This allows the Transformer to capture **richer relationships** than a single attention mechanism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18267a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0a0eb70",
   "metadata": {},
   "source": [
    "### Q20. Which technique is commonly used in Seq2Seq models to handle variable-length output sequences? \n",
    "1. Padding \n",
    "2. Masking \n",
    "3. Beam search \n",
    "4. Redularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0941988",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Masking**\n",
    "\n",
    "In **Seq2Seq models** (especially with attention or Transformers):\n",
    "\n",
    "* **Masking** is used so the model knows **which positions are valid** and which are just padding.\n",
    "* This prevents the model from attending to **padded tokens** and ensures that decoding stops correctly for **variable-length outputs**.\n",
    "\n",
    "**Padding** is applied to inputs for batching, but **masking** is the key technique that tells the model to **ignore padded parts** during training and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4f6b9e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5013c420",
   "metadata": {},
   "source": [
    "### Q21. What is the primary benefit of using the Transformer architecture over traditional RNNs? \n",
    "1. Faster computation and parallelization \n",
    "2. Fewer parameters \n",
    "3. Simpler implementation \n",
    "4. Better performance on short sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d638f39",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**1. Faster computation and parallelization**\n",
    "\n",
    "The **Transformer architecture** eliminates recurrence (RNNs, LSTMs, GRUs) and instead uses **self-attention**, which:\n",
    "\n",
    "* Processes all tokens in a sequence **in parallel** (RNNs process step by step).\n",
    "* Enables much **faster training on GPUs/TPUs**.\n",
    "* Handles **long-range dependencies** more effectively.\n",
    "\n",
    "This parallelism is the **key advantage** that made Transformers dominant in NLP and beyond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acb730",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db11bb87",
   "metadata": {},
   "source": [
    "### Q22. Which gate in the LSTM cell directly controls how much of the previous memory should be retained? \n",
    "1. Input gate \n",
    "2. Output gate \n",
    "3. Forget gate \n",
    "4. Memory gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eed026",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Forget gate** \n",
    "\n",
    "In an **LSTM cell**:\n",
    "\n",
    "* **Forget gate** → decides how much of the **previous cell state (memory)** should be **retained or discarded**.\n",
    "* **Input gate** → controls how much new information to add.\n",
    "* **Output gate** → controls how much of the cell state is exposed as the hidden state.\n",
    "\n",
    "Thus, the **forget gate** is the one that **directly manages retention of past memory**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f07ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4b0f9ff",
   "metadata": {},
   "source": [
    "### Q23. In the Transformer model, what is the significance of using layer normalization? \n",
    "1. It normalizes the input data \n",
    "2. It accelerates model convergence by stabilizing activations \n",
    "3. It adds non-linearity to the model \n",
    "4. It reduces the model size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34abdd5f",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. It accelerates model convergence by stabilizing activations**\n",
    "\n",
    "In **Transformers**, **layer normalization** is applied after sub-layers (like attention and feed-forward networks) to:\n",
    "\n",
    "* Stabilize activations across features.\n",
    "* Prevent exploding/vanishing values.\n",
    "* Make training more stable and **speed up convergence**.\n",
    "\n",
    "Unlike batch normalization, **layer norm works better with variable sequence lengths** and is well-suited for NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b82294",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ce767cb",
   "metadata": {},
   "source": [
    "### Q24. Which of the following is true about the Additive Attention mechanism? \n",
    "1. It is more computationally efficient than multiplicative attention \n",
    "2. It involves a linear transformation of the query and key vectors \n",
    "3. It is used exclusively in RNNs \n",
    "4. It uses the softmax function to compute attention weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6372ceef",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**4. It uses the softmax function to compute attention weights**\n",
    "\n",
    "About **Additive Attention (Bahdanau Attention)**:\n",
    "\n",
    "* It applies **linear transformations** on the query and key vectors, then combines them with a feedforward network to produce alignment scores.\n",
    "* These scores are passed through a **softmax** to obtain attention weights.\n",
    "* Compared to multiplicative (dot-product) attention, it is **less computationally efficient** but works better for small dimensions.\n",
    "\n",
    "So, statement **4** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291e5c72",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3cb12c4",
   "metadata": {},
   "source": [
    "### Q25. How does the Transformer model capture positional information in sequences? \n",
    "1. Through recurrent connections \n",
    "2. Using positional encodings added to the input embeddings \n",
    "3. By using special tokens for start and end of sequence \n",
    "4. By using convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8034ef0",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Using positional encodings added to the input embeddings**\n",
    "\n",
    "Since the **Transformer** has **no recurrence or convolution**, it doesn’t inherently know the order of tokens.\n",
    "\n",
    "To capture **sequence order**, it adds **positional encodings** (sine & cosine functions or learned embeddings) to the input embeddings.\n",
    "\n",
    "This allows the model to understand token positions like **first, second, third**, etc., within the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ecaba6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e449b0a1",
   "metadata": {},
   "source": [
    "### Q26. Which problem does the GRU architecture solve that is often encountered in vanilla RNNs? \n",
    "1. Overfitting \n",
    "2. Vanishing gradient problem \n",
    "3. Insufficient model capacity \n",
    "4. High computational cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af51a198",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Vanishing gradient problem**\n",
    "\n",
    "* **Vanilla RNNs** struggle with **long sequences** because gradients shrink during backpropagation, making it hard to learn long-term dependencies.\n",
    "* **GRUs (Gated Recurrent Units)** use **update and reset gates** to regulate information flow, which helps preserve gradients and capture long-range dependencies.\n",
    "\n",
    "Thus, like LSTMs, GRUs were designed to **solve the vanishing gradient problem**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa5cf8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f65ffdd9",
   "metadata": {},
   "source": [
    "### Q27. What is the role of the output gate in an LSTM cell? \n",
    "1. It determines how much of the cell state should be output as the hidden state \n",
    "2. It decides how much information should be forgotten \n",
    "3. It updates the memory cell \n",
    "4. It normalizes the output sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce34dfb",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**1. It determines how much of the cell state should be output as the hidden state**\n",
    "\n",
    "In an **LSTM cell**:\n",
    "\n",
    "* **Forget gate** → decides what to erase from memory.\n",
    "* **Input gate** → decides what new information to add.\n",
    "* **Output gate** → decides how much of the **cell state** is revealed as the **hidden state (hₜ)** for the next time step and for predictions.\n",
    "\n",
    "So the **output gate controls exposure of memory to the outside**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95d1684",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af123459",
   "metadata": {},
   "source": [
    "### Q28. Why is the Transformer architecture considered more scalable than RNNs for long sequences? \n",
    "1. It uses fewer parameters \n",
    "2. It processes sequences in parallel rather than sequentially \n",
    "3. It has a simpler implementation \n",
    "4. It is specifically designed for short sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdae243",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. It processes sequences in parallel rather than sequentially**\n",
    "\n",
    "* **RNNs/LSTMs/GRUs** process input **step by step**, which makes them **slow** and hard to scale for long sequences.\n",
    "* **Transformers** use **self-attention**, allowing **all tokens to be processed simultaneously** → enabling **parallelization on GPUs/TPUs**.\n",
    "* This parallelism makes Transformers **far more scalable** for **long sequences** compared to RNNs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a535b9bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42fca8c9",
   "metadata": {},
   "source": [
    "### Q29. Which of the following does the \"query\" vector represent in the context of attention mechanisms? \n",
    "1. The context vector for the entire sequence \n",
    "2. A vector that is compared against keys to compute attention scores \n",
    "3. The input to the encoder \n",
    "4. The output of the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8706a453",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. A vector that is compared against keys to compute attention scores** \n",
    "\n",
    "In **attention mechanisms**:\n",
    "\n",
    "* **Query (Q)** → represents what we are looking for (e.g., the current decoder state).\n",
    "* **Key (K)** → represents the attributes of the input sequence elements.\n",
    "* **Value (V)** → contains the actual information to be aggregated.\n",
    "\n",
    "The **query** is matched against the **keys** to produce **attention scores**, which are then used to weight the **values**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc97eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db3ee845",
   "metadata": {},
   "source": [
    "### Q30. In a Transformer model, how does self-attention differ from cross-attention? \n",
    "1. Self-attention only considers the query sequence itself, while cross-attention involves both  the query and a separate context \n",
    "2. Self-attention is used only during inference, while cross-attention is used during training \n",
    "3. Self-training computes attention between different models, while cross-attention is withing a single model \n",
    "4. Self-attention involves multiple sequence, while cross-attention is limited to a single sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594026c4",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**1. Self-attention only considers the query sequence itself, while cross-attention involves both the query and a separate context**\n",
    "\n",
    "* **Self-attention** → Query (Q), Key (K), and Value (V) come from the **same sequence**. Example: encoder self-attention lets each token attend to all tokens in the input.\n",
    "* **Cross-attention** → Q comes from the **decoder**, while K and V come from the **encoder output**. Example: in machine translation, the decoder attends to the encoder’s representation of the source sentence.\n",
    "\n",
    "This is how the Transformer decoder connects input (source) and output (target) sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef7e79",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88671e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924496a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e8420",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
