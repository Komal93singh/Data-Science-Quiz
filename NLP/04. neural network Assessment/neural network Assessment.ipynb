{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74af63b",
   "metadata": {},
   "source": [
    "# Quiz : neural network Assessment\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa115b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa152773",
   "metadata": {},
   "source": [
    "### Q1. 1. What is the primary function of an RNN? 1. Store large amounts of data 2. Process sequences of data 3. Perform matrix multiplication 4. Generate random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c8f7d2",
   "metadata": {},
   "source": [
    "The correct answer is **2. Process sequences of data**\n",
    "\n",
    "RNNs (Recurrent Neural Networks) are specifically designed to handle **sequential or time-series data** by maintaining a hidden state that carries information from previous steps to help predict or classify the next ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e981d8ed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5e14e67",
   "metadata": {},
   "source": [
    "### Q2. Which component in an LSTM controls how much of the cell state is passed to the next hidden state? \n",
    "1. Input Gate \n",
    "2. Forget Gate \n",
    "3. Output Gate \n",
    "4. Reset Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cae5ca",
   "metadata": {},
   "source": [
    "The correct answer is **3. Output Gate**\n",
    "\n",
    "In an LSTM, the **output gate** decides **how much of the cell state should be exposed as the hidden state** at the current time step, which is then passed to the next layer or the next time step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409b4fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1897d683",
   "metadata": {},
   "source": [
    "### Q3. Which gate in a GRU determines which parts of the  previous hidden state should be reset? \n",
    "1. Input Gate \n",
    "2. Forget Gate \n",
    "3. Reset Gate \n",
    "4. Update Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb225c9a",
   "metadata": {},
   "source": [
    "The correct answer is **3. Reset Gate**\n",
    "\n",
    "In a GRU (Gated Recurrent Unit), the **reset gate** controls **how much of the previous hidden state to forget** when calculating the new candidate hidden state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dfc77d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "013c7d05",
   "metadata": {},
   "source": [
    "### Q4. What is the primary advantage of LSTMs over standard RNNs? \n",
    "1. Faster training \n",
    "2. Simpler architecture \n",
    "3. Better at learning long-term dependencies \n",
    "4. Lower computational cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603ff54",
   "metadata": {},
   "source": [
    "The correct answer is **3. Better at learning long-term dependencies**\n",
    "\n",
    "LSTMs use **gates and a cell state** to prevent the vanishing gradient problem, making them much better than standard RNNs at capturing long-term relationships in sequential data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031633e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a564a2b",
   "metadata": {},
   "source": [
    "### Q5. Which of the following is NOT a gate in an LSTM? \n",
    "1. Input Gate \n",
    "2. Forget Gate \n",
    "3. Reset Gate \n",
    "4. Output Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8aae6e",
   "metadata": {},
   "source": [
    "The correct answer is **3. Reset Gate** \n",
    "\n",
    "The **reset gate** exists in a **GRU**, not in an LSTM.\n",
    "LSTMs have **three gates**: **Input Gate**, **Forget Gate**, and **Output Gate**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d7962b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e93c7a01",
   "metadata": {},
   "source": [
    "### Q6. What is the purpose of the cell state in an LSTM? \n",
    "1. To control the output of the LSTM \n",
    "2. To store long-term memory \n",
    "3. To reset the hidden state \n",
    "4. To update the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c17760",
   "metadata": {},
   "source": [
    "The correct answer is **2. To store long-term memory**\n",
    "\n",
    "In an LSTM, the **cell state** acts like a conveyor belt, carrying **long-term information** through the sequence with minimal changes, allowing the network to remember important patterns over long time spans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aea9f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0bd9916",
   "metadata": {},
   "source": [
    "### Q7. Which of the following is a challenge associated with RNNs? \n",
    "1. Vanishing gradients \n",
    "2. High computational cost \n",
    "3. Inability to process sequences \n",
    "4. Limited application areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1a3a7e",
   "metadata": {},
   "source": [
    "The correct answer is **1. Vanishing gradients**\n",
    "\n",
    "RNNs often suffer from the **vanishing gradient problem** during training, making it difficult for them to learn long-term dependencies in sequences. This is one of the main reasons LSTMs and GRUs were developed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023112f9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72e1846d",
   "metadata": {},
   "source": [
    "### Q8. In GRUs, which gate controls how much of the previous hidden state should be carried forward to the current hidden state? \n",
    "1. Input Gate \n",
    "2. Forget Gate \n",
    "3. Reset Gate \n",
    "4. Update Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f636ca0e",
   "metadata": {},
   "source": [
    "The correct answer is **4. Update Gate**\n",
    "\n",
    "In a GRU, the **update gate** decides **how much of the previous hidden state to keep** and pass along to the current hidden state, balancing between old information and new input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec229c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6322674",
   "metadata": {},
   "source": [
    "### Q9. Which of the following models is best suited for real-time applications due to its faster training and lower computational cost? \n",
    "1. RNN \n",
    "2. LSTM \n",
    "3. GRU \n",
    "4. CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e214039",
   "metadata": {},
   "source": [
    "The correct answer is **3. GRU**\n",
    "\n",
    "GRUs have **fewer gates** and a **simpler structure** than LSTMs, making them **faster to train** and **less computationally expensive**, which is great for **real-time applications**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18bb36",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20771536",
   "metadata": {},
   "source": [
    "### Q10. Which activation function is commonly used in RNNs to introduce non-linearity? \n",
    "1. Sigmoid \n",
    "2. Tanh \n",
    "3. ReLU \n",
    "4. Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec507f5",
   "metadata": {},
   "source": [
    "The correct answer is **2. Tanh**\n",
    "\n",
    "RNNs commonly use the **tanh** activation function in their hidden layers to introduce non-linearity and keep values within a range of **-1 to 1**, which helps stabilize training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec13cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77c4a17f",
   "metadata": {},
   "source": [
    "### Q11. In a Bi-LSTM, how is information processed differently compared to a standard LSTM? \n",
    "1. Only in the forward direction \n",
    "2. Only in the backward direction \n",
    "3. In both forward and backward directions \n",
    "4. In random order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420762eb",
   "metadata": {},
   "source": [
    "The correct answer is **3. In both forward and backward directions**\n",
    "\n",
    "A **Bi-LSTM** processes the sequence **twice** — once from start to end (forward) and once from end to start (backward) — allowing it to capture **context from both past and future** in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917fc26",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9f5ebd4",
   "metadata": {},
   "source": [
    "### Q12. What is the key advantage of using a Bi-LSTM over a standard LSTM? \n",
    "1. Faster training \n",
    "2. Ability to capture dependencies in both directions \n",
    "3. Simpler architecture \n",
    "4. Reduced memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac09e7d",
   "metadata": {},
   "source": [
    "The correct answer is **2. Ability to capture dependencies in both directions**\n",
    "\n",
    "A **Bi-LSTM** can learn **past and future context** for each time step, making it especially useful in tasks like **speech recognition, text classification, and machine translation** where context from both directions improves accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1270e9b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c50660e2",
   "metadata": {},
   "source": [
    "### Q13. Which of the following is a component of GRU but not of LSTM? \n",
    "1. Forget Gate \n",
    "2. Cell state \n",
    "3. Reset Gate \n",
    "4. Input GAte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34abdcaf",
   "metadata": {},
   "source": [
    "The correct answer is **3. Reset Gate** \n",
    "\n",
    "The **reset gate** is a unique component of a **GRU**, while LSTMs use separate **input, forget, and output gates** along with a **cell state**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbce279",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39528b7c",
   "metadata": {},
   "source": [
    "### Q14. Which application would benefit most from the use of LSTMs rather GRUs? \n",
    "1. Real-time speech recognition \n",
    "2. Short sequence prediction \n",
    "3. Long document translation \n",
    "4. Simple time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411ad5b",
   "metadata": {},
   "source": [
    "The correct answer is **3. Long document translation** \n",
    "\n",
    "LSTMs are better than GRUs at handling **very long-term dependencies**, making them more suitable for tasks like **translating long documents**, where context needs to be preserved over many time steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62941e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bc8539a",
   "metadata": {},
   "source": [
    "### Q15. In the context of RNNs, what does \"vanishing gradient\" refer to? \n",
    "1. Gradients becoming too large during backpropagation \n",
    "2. Gradients becoming too small during backpropagation \n",
    "3. The model not learning from the data \n",
    "4. Teh model learning too fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bee2ed7",
   "metadata": {},
   "source": [
    "The correct answer is **2. Gradients becoming too small during backpropagation**\n",
    "\n",
    "The **vanishing gradient problem** occurs when gradients shrink toward zero as they are propagated backward through many time steps, making it hard for RNNs to learn long-term dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3017566f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a036e3b",
   "metadata": {},
   "source": [
    "### Q16. What is the primary difference between the Distributed Memory model and the Distributed Bag of Words (DBOW) model in Doc2Vec? \n",
    "1. DM uses embeddings; DBOW uses document embeddings \n",
    "2. DM predicts the surrounding words given the document context; DBOW predicts the document vector given a context word \n",
    "3. DM is faster than DBOW \n",
    "4. DBOW uses a more complex architecture than DM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e7418",
   "metadata": {},
   "source": [
    "The correct answer is **2. DM predicts the surrounding words given the document context; DBOW predicts the document vector given a context word** \n",
    "\n",
    "* **Distributed Memory (DM)** works like Word2Vec’s CBOW — it uses both **context words** and a **document ID** to predict a target word.\n",
    "* **Distributed Bag of Words (DBOW)** works like Word2Vec’s Skip-gram — it uses the **document ID** to predict words in the document, ignoring the order of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50310f0c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77c5e4c6",
   "metadata": {},
   "source": [
    "### Q17. What is a key advantage of using GRU over LSTM? \n",
    "1. GRU is more complex and hence captures more information \n",
    "2. GRU is computationally simpler and faster \n",
    "3. GRU has an additional cell state which improves performance \n",
    "4. GRU uses more memory than LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8aef73",
   "metadata": {},
   "source": [
    "The correct answer is **2. GRU is computationally simpler and faster** \n",
    "\n",
    "GRUs have **fewer gates** (only update and reset) and no separate cell state, making them **less computationally expensive** and **faster to train** than LSTMs, while still performing well on many tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda009f4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1476f2a1",
   "metadata": {},
   "source": [
    "### Q18. Why is the vanishing gradient problem less severe in LSTMs compared to standard RNNs? \n",
    "1. Because LSTMs use a different activation function \n",
    "2. Due to the presence of gating mechanisms that control the flow of information \n",
    "3. Because LSTMs do not backpropagate errors \n",
    "4. LSTMs are not affected by gradient problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70097cd8",
   "metadata": {},
   "source": [
    "The correct answer is **2. Due to the presence of gating mechanisms that control the flow of information** \n",
    "\n",
    "LSTMs use **input, forget, and output gates** along with a **cell state** to regulate how information is stored, updated, and passed along, which helps prevent gradients from shrinking too much during backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e48b1f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "286e3567",
   "metadata": {},
   "source": [
    "### Q19. In the context of RNNs, what does \"exploding gradient\" refer to? \n",
    "1. Gradient becoming too large during backpropagation \n",
    "2. Gradients becoming too small during backpropagation \n",
    "3. The model learning too slowly \n",
    "4. The model learning too fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8912d2c3",
   "metadata": {},
   "source": [
    "The correct answer is **1. Gradient becoming too large during backpropagation**\n",
    "\n",
    "The **exploding gradient problem** happens when gradients grow excessively during backpropagation, causing **unstable training** where weights can blow up to very large values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ca07b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed594e0c",
   "metadata": {},
   "source": [
    "### Q20. Which statement best describes the purpose of an Update Gate in a GRU? \n",
    "1. It resets the hidden state \n",
    "2. It determine how much of the past information to carry forward \n",
    "3. It controls the output of the GRU \n",
    "4. It stores long-term information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a8281b",
   "metadata": {},
   "source": [
    "The correct answer is **2. It determines how much of the past information to carry forward**\n",
    "\n",
    "In a GRU, the **update gate** balances **old information from the previous hidden state** with **new information from the current input**, deciding how much of the past should be preserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a54b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ca16958",
   "metadata": {},
   "source": [
    "### Q21. What is the major disadvantage of using standard RNNs for tasks requiring long-term dependencies? \n",
    "1. High computational cost \n",
    "2. Difficulty in learning long-term dependencies due to vanishing gradients \n",
    "3. Inability to process sequential data \n",
    "4. Lack of flexibility in architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e420fa",
   "metadata": {},
   "source": [
    "The correct answer is **2. Difficulty in learning long-term dependencies due to vanishing gradients**\n",
    "\n",
    "Standard RNNs struggle with **remembering information over many time steps** because gradients shrink during backpropagation, making it hard to capture long-term patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f74dd11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f0b832e",
   "metadata": {},
   "source": [
    "### Q22. How does a Bi-LSTM differ from a unidirectional LSTM in handling sequence data? \n",
    "1. It processes the sequence in reverse order \n",
    "2. It processes the sequence in both forward and backward directions\n",
    "simultaneously \n",
    "3. It only uses the output gate for memory retention \n",
    "4. It ignores the hidden state of the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca8381",
   "metadata": {},
   "source": [
    "The correct answer is **2. It processes the sequence in both forward and backward directions simultaneously**\n",
    "\n",
    "A **Bi-LSTM** runs two LSTMs:\n",
    "\n",
    "* One processes the sequence **from start to end** (forward).\n",
    "* The other processes it **from end to start** (backward).\n",
    "  This allows the model to **capture context from both past and future** at each time step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59483a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e85722af",
   "metadata": {},
   "source": [
    "### Q23. Which of the following describes the Distributed Bag of Words (DBOW) model in Doc2Vec? \n",
    "1. It predicts the document vector given a context word \n",
    "2. It predicts the surrounding words given the document context \n",
    "3. It combines word embeddings with document embeddings \n",
    "4. It is slower but accurate than the Distributed Memory model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a0d28f",
   "metadata": {},
   "source": [
    "The correct answer is **1. It predicts the document vector given a context word**\n",
    "\n",
    "In **DBOW** (Distributed Bag of Words) Doc2Vec:\n",
    "\n",
    "* The model uses the **document ID** (vector) to predict words in the document.\n",
    "* It ignores the order of words, focusing instead on learning a **single vector representation** for the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13644c35",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f9f2619",
   "metadata": {},
   "source": [
    "### Q24. In LSTM, which gate is responsible for determining which information to discard from the cell state? \n",
    "1. Input Gate \n",
    "2. Output Gate \n",
    "3. Forget Gate \n",
    "4. Update Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce370e31",
   "metadata": {},
   "source": [
    "The correct answer is **3. Forget Gate**\n",
    "\n",
    "In an LSTM, the **forget gate** decides **which parts of the cell state to remove** by assigning values between 0 (completely forget) and 1 (completely keep) to each piece of information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128868e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7149447",
   "metadata": {},
   "source": [
    "### Q25. Which of the following is a key characteristic of Bi-LSTM? \n",
    "1. It uses only the forward sequence of data \n",
    "2. It uses only the backward sequence of data \n",
    "3. It processes sequence in both forward and backward directions \n",
    "4. It is less effective than unidirectional LSTM for sequence processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09260a4d",
   "metadata": {},
   "source": [
    "The correct answer is **3. It processes sequence in both forward and backward directions** \n",
    "\n",
    "A **Bi-LSTM** combines outputs from **forward** and **backward** passes, enabling it to capture **context from both past and future** in sequence data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99396bf0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "144ecfe7",
   "metadata": {},
   "source": [
    "### Q26. What is the significance of the Reset Gate in a GRU? \n",
    "1. It resets the output to zero \n",
    "2. It decides which parts of the hidden state to reset before combining with new input \n",
    "3. It increases the learning rate of the GRU \n",
    "4. It adjusts the input to the GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c7b058",
   "metadata": {},
   "source": [
    "The correct answer is **2. It decides which parts of the hidden state to reset before combining with new input** \n",
    "\n",
    "In a GRU, the **reset gate** controls how much of the **previous hidden state** should be ignored when computing the **candidate hidden state**, allowing the model to focus on new information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e12e079",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4761f934",
   "metadata": {},
   "source": [
    "### Q27. In LSTM, what role does the Cell State play in retaining information? \n",
    "1. It directly stores output values \n",
    "2. It acts as a conveyor belt, passing information along unchanged \n",
    "3. It eliminates unnecessary information before passing it to the next layer \n",
    "4. It stores temporary information that is discarded after each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcce22c",
   "metadata": {},
   "source": [
    "The correct answer is **2. It acts as a conveyor belt, passing information along unchanged**\n",
    "\n",
    "In an LSTM, the **cell state** runs through the network with only minor changes (controlled by gates), allowing it to **retain important long-term information** across many time steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79ea01",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fea9b43",
   "metadata": {},
   "source": [
    "### Q28. Why might GRUs be preferred over LSTMs in some scenarios? \n",
    "1. They have more gates and thus more flexibility \n",
    "2. They are simpler and require less computational resources \n",
    "3. They are more accurate for all tasks \n",
    "4. They have a more complex architecture that captures more nuances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eff4be",
   "metadata": {},
   "source": [
    "The correct answer is **2. They are simpler and require less computational resources**\n",
    "\n",
    "GRUs have **fewer gates** (update and reset) and no separate cell state, which makes them **faster to train** and **less memory-intensive** than LSTMs, while still performing well in many tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f4566",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63193aa0",
   "metadata": {},
   "source": [
    "### Q29. Which of the following best describes the vanishing gradient problem? \n",
    "1. The network's weights become too large \n",
    "2. The gradients used to update the weights become very small, slowing down learning \n",
    "3. The loss function fails to coverage \n",
    "4. The model overfits to the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c7ff4",
   "metadata": {},
   "source": [
    "The correct answer is **2. The gradients used to update the weights become very small, slowing down learning**\n",
    "\n",
    "The **vanishing gradient problem** occurs when backpropagating through many layers or time steps causes gradients to shrink toward zero, making it hard for the network to learn long-term patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd19826",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4135f43b",
   "metadata": {},
   "source": [
    "### Q30. Which of the following applications is most suitable for an LSTM rather than a GRU? \n",
    "1. Short sequence sentiment analysis \n",
    "2. Real-time chatbots responses \n",
    "3. Long document translation \n",
    "4. Simple time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcdf3ff",
   "metadata": {},
   "source": [
    "The correct answer is **3. Long document translation**\n",
    "\n",
    "LSTMs handle **long-term dependencies** better than GRUs, making them ideal for tasks like **long document translation**, where maintaining context over many time steps is essential.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ff599",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30835667",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751acf2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13275677",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
