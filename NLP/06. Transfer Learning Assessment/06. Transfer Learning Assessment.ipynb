{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f72bd2",
   "metadata": {},
   "source": [
    "# Quiz : Transfer Learning Assessment\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0757f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd993032",
   "metadata": {},
   "source": [
    "### Q1. What is a Sequence to Sequence model commonly used for? \n",
    "1. Image Classification \n",
    "2. Machine translation \n",
    "3. Object detection \n",
    "4. Video processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a941a",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. Machine translation**\n",
    "\n",
    "A Sequence to Sequence (Seq2Seq) model is designed to take a sequence of inputs (like words in a sentence) and generate a sequence of outputs (like words in another language). While machine translation (e.g., English → French) is the most common application, Seq2Seq models are also used in text summarization, chatbots, and speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9897688",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf4a2eb4",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "**Q1. How does a Seq2Seq model handle different input and output sequence lengths?**\n",
    "A Seq2Seq model uses an **encoder-decoder architecture**. The encoder processes the input sequence into a fixed-size context vector (or series of hidden states), and the decoder generates the output sequence step by step. Since it works token by token, input and output lengths don’t need to match (e.g., \"I am happy\" → \"Je suis content\"). Attention mechanisms further improve this by allowing the decoder to focus on different parts of the input dynamically.\n",
    "\n",
    "**Q2. Why is attention important in Seq2Seq models?**\n",
    "Without attention, the entire input sequence gets compressed into a single vector, which may lose important information in long sentences. Attention allows the decoder to \"look back\" at relevant parts of the input sequence when predicting each token, leading to better translation accuracy and more fluent results.\n",
    "\n",
    "**Q3. Besides machine translation, what are some real-world applications of Seq2Seq models?**\n",
    "\n",
    "* **Text summarization** (condensing long articles into short summaries)\n",
    "* **Speech recognition** (mapping audio frames to text)\n",
    "* **Chatbots & conversational AI** (input query → response generation)\n",
    "* **Image captioning** (image features → descriptive sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87026379",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e540c631",
   "metadata": {},
   "source": [
    "### Q2. In a Sequence to Sequence model, what component is responsible for generating the output sequence? \n",
    "1. Encoder \n",
    "2. Decoder \n",
    "3. Attention mechanism \n",
    "4. Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7876025",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. Decoder**\n",
    "\n",
    "In a **Sequence to Sequence (Seq2Seq) model**, the **encoder** processes the input sequence and converts it into a context vector (or hidden states), while the **decoder** takes this representation and **generates the output sequence step by step**.\n",
    "\n",
    "* **Encoder** → Encodes the input sequence into hidden states\n",
    "* **Decoder** → Uses those hidden states (and possibly attention) to generate the output sequence\n",
    "* **Attention mechanism** → Helps the decoder focus on different parts of the input sequence\n",
    "* **Transformer** → A model architecture that uses self-attention for both encoding and decoding, but within a Seq2Seq framework, the **decoder** is still the component generating the output\n",
    "\n",
    "Quick Example:\n",
    "\n",
    "* Input: \"I am happy\"\n",
    "* Encoder → Encodes meaning into hidden states\n",
    "* Decoder → Generates: \"Je suis content\" (French)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa80dea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6938fc13",
   "metadata": {},
   "source": [
    "### Q3. What is the primary role of the encoder in a Sequence to Sequence model? \n",
    "1. To generate the output sequence \n",
    "2. To process the input sequence \n",
    "3. To calculate attention scores \n",
    "4. To improve model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6798cdd9",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. To process the input sequence**\n",
    "\n",
    "In a **Sequence to Sequence (Seq2Seq) model**:\n",
    "\n",
    "* The **encoder** reads the input sequence (e.g., an English sentence) and transforms it into a hidden representation (context vector or sequence of states).\n",
    "* This representation captures the **meaning and structure** of the input, which the **decoder** then uses to generate the output sequence.\n",
    "\n",
    "Breakdown of the options:\n",
    "\n",
    "1. **To generate the output sequence** → That’s the **decoder’s job**.\n",
    "2. **To process the input sequence** → Correct! The encoder encodes the input into useful features.\n",
    "3. **To calculate attention scores** → Attention is a **separate mechanism** that works between encoder and decoder.\n",
    "4. **To improve model accuracy** → Not its direct role (though processing input properly contributes indirectly).\n",
    "\n",
    "Example:\n",
    "Input: `\"I love AI\"`\n",
    "\n",
    "* **Encoder:** Converts `\"I love AI\"` into hidden states like `[h1, h2, h3]`.\n",
    "* **Decoder:** Uses these hidden states to produce `\"J'adore l'IA\"` in French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff06400",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb0b506c",
   "metadata": {},
   "source": [
    "### Q4. In the context of encoder-decoder models, what does the decoder do? \n",
    "1. Processes the input sequence \n",
    "2. Generates the output sequence \n",
    "3. Encodes the input features \n",
    "4. Maps input directly to output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598410e5",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. Generates the output sequence**\n",
    "\n",
    "In an **encoder–decoder model (Seq2Seq)**:\n",
    "\n",
    "* **Encoder** → Processes the **input sequence** and converts it into a hidden representation (context vector or sequence of states).\n",
    "* **Decoder** → Takes that representation (plus attention, if used) and **generates the output sequence token by token**.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **Processes the input sequence** → That’s the encoder’s job.\n",
    "2. **Generates the output sequence** → Correct!\n",
    "3. **Encodes the input features** → That’s also the encoder’s role.\n",
    "4. **Maps input directly to output** → That would be more like a **simple feed-forward model** without encoder-decoder architecture.\n",
    "\n",
    "Example:\n",
    "Input: `\"How are you?\"`\n",
    "\n",
    "* Encoder → Converts to hidden representation `[h1, h2, h3, h4]`\n",
    "* Decoder → Generates `\"Comment ça va ?\"` (French) **one word at a time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad8b20",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4eaf4eb",
   "metadata": {},
   "source": [
    "### Q5. What is the primary function of the attention mechanism in sequence models? \n",
    "1. To increase the model's learning rate \n",
    "2. To focus on relevant parts of the input sequence \n",
    "3. To reduce overfitting \n",
    "4. To regularize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f793315",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. To focus on relevant parts of the input sequence**\n",
    "\n",
    "In **sequence models (Seq2Seq, Transformers, etc.)**, the **attention mechanism** allows the decoder to **look back at different parts of the input sequence dynamically** instead of relying on a single fixed context vector.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **To increase the model's learning rate** → Learning rate is a training hyperparameter, not related to attention.\n",
    "2. **To focus on relevant parts of the input sequence** → Correct!\n",
    "3. **To reduce overfitting** → That’s done by dropout, regularization, etc.\n",
    "4. **To regularize the model** → Not its main purpose (though it may indirectly help).\n",
    "\n",
    "Example:\n",
    "When translating:\n",
    "Input: `\"The cat sat on the mat\"`\n",
    "\n",
    "* While predicting `\"chat\"` (French for cat), the decoder **attends** mostly to `\"cat\"`.\n",
    "* While predicting `\"tapis\"` (mat), it focuses more on `\"mat\"`.\n",
    "\n",
    "This dynamic **focus mechanism** improves translation, summarization, and many other NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ba76f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62139e72",
   "metadata": {},
   "source": [
    "### Q6. Which type of attention mechanism is commonly used in Transformer models? \n",
    "1. Additive \n",
    "2. Multiplicative \n",
    "3. Self-attention \n",
    "4. Cross-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf18d4e",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**3. Self-attention**\n",
    "\n",
    "In **Transformer models** (like BERT, GPT, etc.):\n",
    "\n",
    "* **Self-attention** is the core mechanism → it allows each token in a sequence to **attend to all other tokens** in the same sequence, capturing dependencies regardless of distance.\n",
    "* This is what makes Transformers powerful for handling long-range relationships in text.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **Additive attention** → Used in early Seq2Seq models (Bahdanau attention).\n",
    "2. **Multiplicative attention** → Also called dot-product attention (Luong attention), used before Transformers but less central.\n",
    "3. **Self-attention** → Correct! Fundamental building block of Transformers.\n",
    "4. **Cross-attention** → Used in **encoder–decoder Transformers** (like in machine translation), but the *most common* attention in Transformers is **self-attention**.\n",
    "\n",
    "Example of self-attention:\n",
    "Sentence: `\"The cat sat on the mat\"`\n",
    "\n",
    "* When processing `\"mat\"`, the model can attend to `\"cat\"` and `\"sat\"` to understand context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6520b34",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43668aef",
   "metadata": {},
   "source": [
    "### Q7. What is the key benefit of self-attention in models like the Transformer? \n",
    "1. Reduces the need for large datasets \n",
    "2. Increases training speed \n",
    "3. Captures dependencies regardless of distance in the sequence \n",
    "4. Simplifies the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b02b52",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**3. Captures dependencies regardless of distance in the sequence**\n",
    "\n",
    "In models like the **Transformer**, the **key benefit of self-attention** is that it allows every token in a sequence to directly attend to every other token. This means:\n",
    "\n",
    "* Long-range dependencies (e.g., subject ↔ verb agreement across many words) are captured more effectively than in RNNs or LSTMs, which struggle with long sequences.\n",
    "* Context is learned in parallel across the sequence, not step by step.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **Reduces the need for large datasets** → Transformers actually need **more** data to train well.\n",
    "2. **Increases training speed** → Not exactly. Self-attention enables parallelism (faster than RNNs), but that’s a side benefit, not the *key*.\n",
    "3. **Captures dependencies regardless of distance** → Correct! This is the **main strength** of self-attention.\n",
    "4. **Simplifies the model architecture** → Transformers are actually **more complex** than RNNs.\n",
    "\n",
    "Example:\n",
    "Sentence: `\"The book that you gave me yesterday was amazing.\"`\n",
    "\n",
    "* In predicting `\"was\"`, the model can **directly attend to** `\"book\"`, even though many words are in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb3dfe2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a04659f9",
   "metadata": {},
   "source": [
    "### Q8. In Self-attention, each word in a sequence is related to: 1. Only the previous word 2. All other words in the sequence 3. Only the next word 4. The first word in the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2defb0",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. All other words in the sequence**\n",
    "\n",
    "In **self-attention**, each word (or token) in a sequence creates queries, keys, and values, and then it computes **attention scores with every other word** in the same sequence.\n",
    "\n",
    "This means every word can \"look at\" (attend to) all others, regardless of position.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **Only the previous word** → That’s how **RNNs** work (sequential).\n",
    "2. **All other words in the sequence** → Correct!\n",
    "3. **Only the next word** → Not true — attention isn’t limited like that.\n",
    "4. **The first word in the sequence** → Not correct — it’s not restricted.\n",
    "\n",
    "Example:\n",
    "Sentence: `\"The cat sat on the mat\"`\n",
    "\n",
    "* For `\"mat\"`, self-attention can look at `\"The\"`, `\"cat\"`, `\"sat\"`, `\"on\"`, and `\"the\"` simultaneously to understand context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e97aba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7e979db",
   "metadata": {},
   "source": [
    "### Q9. What key component in Transformers allows for parallelization of Training? \n",
    "1. Recurrent connections \n",
    "2. Attention mechanism \n",
    "3. LSTM cells \n",
    "4. Dropout layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3537da1a",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. Attention mechanism**\n",
    "\n",
    "In **Transformers**, the **attention mechanism (specifically self-attention)** enables the model to process all tokens in a sequence **in parallel**, unlike RNNs and LSTMs that process tokens sequentially.\n",
    "\n",
    "This parallelization is what makes Transformers much faster to train on large datasets.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **Recurrent connections** → Found in RNNs/LSTMs, but they **prevent parallelization** since they depend on previous states.\n",
    "2. **Attention mechanism** →  Correct! Enables **parallel computation** across all tokens.\n",
    "3. **LSTM cells** → Part of RNNs, not used in Transformers.\n",
    "4. **Dropout layers** → Used for regularization, not parallelization.\n",
    "\n",
    "Example:\n",
    "\n",
    "* **RNN:** Must process `\"I → love → AI\"` one token at a time.\n",
    "* **Transformer with self-attention:** Processes `\"I, love, AI\"` **all at once**, while still learning dependencies among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d5e60e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f87038a",
   "metadata": {},
   "source": [
    "### Q10. Which of the following is a significant advantage of Transformers over RNNs? \n",
    "1. Lower computational cost \n",
    "2. Sequential processing of data \n",
    "3. Handling long-range dependencies effectively \n",
    "4. Simplified architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6a427",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**3. Handling long-range dependencies effectively**\n",
    "\n",
    "Transformers solve one of the biggest problems in RNNs and LSTMs — **capturing dependencies between words that are far apart** in a sequence. Thanks to **self-attention**, a token can directly attend to any other token, no matter the distance.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **Lower computational cost** → Not true — Transformers are computationally **heavier**, though parallelizable.\n",
    "2. **Sequential processing of data** → That’s how **RNNs** work, not Transformers.\n",
    "3. **Handling long-range dependencies effectively** → Correct! Major strength of Transformers.\n",
    "4. **Simplified architecture** → Transformers are actually **more complex** than RNNs.\n",
    "\n",
    "Example:\n",
    "Sentence: *“The book I bought last week at the store was amazing.”*\n",
    "\n",
    "* An RNN struggles to connect *“book”* with *“was”* because of the long gap.\n",
    "* A Transformer links them directly through **self-attention**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d27fa8e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c64ddfc6",
   "metadata": {},
   "source": [
    "### Q11. What is the pre-training objective in BERT that involves predicting missing words? \n",
    "1. Next sentence prediction \n",
    "2. Masked language model \n",
    "3. Word2Vec \n",
    "4. GloVe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc24ae",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. Masked language model**\n",
    "\n",
    "In **BERT (Bidirectional Encoder Representations from Transformers)**, one of the pre-training objectives is the **Masked Language Model (MLM)**:\n",
    "\n",
    "* Some words in the input sentence are **randomly masked** (e.g., replaced with `[MASK]`).\n",
    "* The model is trained to **predict the missing words** using the surrounding context.\n",
    "\n",
    "Breakdown of options\n",
    "\n",
    "1. **Next sentence prediction** → Another BERT objective, but it’s about predicting whether one sentence follows another, not about missing words.\n",
    "2. **Masked language model** → Correct! Predicts missing words.\n",
    "3. **Word2Vec** → A different word embedding technique, not BERT’s pre-training task.\n",
    "4. **GloVe** → Another embedding method, also not used in BERT pre-training.\n",
    "\n",
    "Example:\n",
    "Input: `\"The cat sat on the [MASK].\"`\n",
    "\n",
    "* BERT predicts `\"mat\"` as the missing word using context from both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2fb8b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d341a4f",
   "metadata": {},
   "source": [
    "### Q12. BERT is an example of which type of neural network architecture? \n",
    "1. Convolutional Neural Network (CNN) \n",
    "2. Recurrent Neural Network (RNN) \n",
    "3. Transformer \n",
    "4. Restricted Boltzmann Machine (RBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e196002a",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**3. Transformer**\n",
    "\n",
    "BERT (**Bidirectional Encoder Representations from Transformers**) is built entirely on the **Transformer architecture** — specifically, it uses **only the encoder stack** of the Transformer.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **CNN** → Used in image processing, not BERT.\n",
    "2. **RNN** → Earlier NLP models used RNNs/LSTMs, but BERT replaced them with Transformers.\n",
    "3. **Transformer** → Correct! (Encoder-based Transformer model).\n",
    "4. **RBM** → Restricted Boltzmann Machines are older generative models, not used in BERT.\n",
    "\n",
    "Key Point:\n",
    "\n",
    "* **BERT** = Transformer **Encoder** (bidirectional).\n",
    "* **GPT** = Transformer **Decoder** (unidirectional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b637cca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a802d153",
   "metadata": {},
   "source": [
    "### Q13. Which BERT variant is known for being smaller and faster while retaining performance? \n",
    "1. RoBERTa \n",
    "2. ALBERT \n",
    "3. BERT Large \n",
    "4. DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02252b7",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**4. DistilBERT** \n",
    "\n",
    "**DistilBERT** is a **smaller, faster, and lighter version of BERT**, created using *knowledge distillation*. It retains about **95% of BERT’s performance** while being **40% smaller** and **60% faster** — making it great for deployment on resource-constrained environments.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **RoBERTa** → A robustly optimized BERT, but not smaller — it’s actually *larger and better trained*.\n",
    "2. **ALBERT** → Reduces parameters via **parameter sharing and factorization**, but not specifically “distilled.”\n",
    "3. **BERT Large** → Bigger and slower, opposite of smaller/faster.\n",
    "4. **DistilBERT** → Correct! Compact version of BERT.\n",
    "\n",
    "Quick Analogy:\n",
    "\n",
    "* **BERT** = Big full book.\n",
    "* **DistilBERT** = A compact summary of the book, keeping almost all important points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56992d1c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12e407e3",
   "metadata": {},
   "source": [
    "### Q14. Which BERT model is designed for improved efficiency and reduced parameter size? \n",
    "1. RoBERTa \n",
    "2. ALBERT \n",
    "3. BERT Base \n",
    "4. GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b73a2e4",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. ALBERT**\n",
    "\n",
    "**ALBERT (A Lite BERT)** was designed to improve efficiency and reduce parameter size while maintaining performance. It achieves this mainly through:\n",
    "\n",
    "* **Factorized embedding parameterization** → reduces the size of the embedding layer.\n",
    "* **Cross-layer parameter sharing** → the same weights are reused across layers, reducing total parameters dramatically.\n",
    "\n",
    "As a result, ALBERT is **much smaller** than BERT but still very powerful.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **RoBERTa** → Optimized training of BERT (more data, longer training), not reduced size.\n",
    "2. **ALBERT** → Correct! Designed for efficiency + reduced parameters.\n",
    "3. **BERT Base** → Standard BERT, not optimized for efficiency.\n",
    "4. **GPT** → A Transformer **decoder** model, not a smaller BERT.\n",
    "\n",
    "Example:\n",
    "\n",
    "* **BERT Large** → ~340 million parameters\n",
    "* **ALBERT Large** → ~18 million parameters (thanks to weight sharing!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909f734",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92153d80",
   "metadata": {},
   "source": [
    "### Q15. What does GPT stand for in GPT models? \n",
    "1. Generalized Processing Transformer \n",
    "2. Generative Pre-trained Transformer \n",
    "3. Gradual Parameter Training \n",
    "4. General Purpose Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b53d48",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. Generative Pre-trained Transformer** \n",
    "\n",
    "**GPT models** are:\n",
    "\n",
    "* **Generative** → They can generate text (not just classify).\n",
    "* **Pre-trained** → First trained on massive text corpora with language modeling objectives.\n",
    "* **Transformer** → Built on the Transformer **decoder** architecture.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **Generalized Processing Transformer** → Not correct.\n",
    "2. **Generative Pre-trained Transformer** → Correct!\n",
    "3. **Gradual Parameter Training** → Not related to GPT.\n",
    "4. **General Purpose Translation** → GPT is not limited to translation; it’s general-purpose text generation.\n",
    "\n",
    "Example:\n",
    "\n",
    "* **Input:** “Once upon a time…”\n",
    "* **GPT Output:** “…there was a curious cat who loved exploring hidden gardens.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c4003",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9785caa2",
   "metadata": {},
   "source": [
    "### Q16. What is the primary training objective of GPT models? \n",
    "1. Next sentence prediction \n",
    "2. Masked language modelling \n",
    "3. Next word prediction \n",
    "4. Sentence ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17877375",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**3. Next word prediction** \n",
    "\n",
    "**GPT (Generative Pre-trained Transformer)** is trained with the **causal language modeling (CLM) objective**, where the model learns to predict the **next word** in a sequence given all previous words.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **Next sentence prediction** → That’s part of **BERT** pre-training, not GPT.\n",
    "2. **Masked language modeling** → Also BERT’s objective (predict missing words).\n",
    "3. **Next word prediction** → Correct! GPT predicts the next token autoregressively.\n",
    "4. **Sentence ordering** → Not a GPT training objective.\n",
    "\n",
    "Example (GPT training):\n",
    "Input: `\"The cat sat on the\"`\n",
    "Target: `\"mat\"`\n",
    "\n",
    "The model is trained to generate `\"mat\"` given the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea17a52",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b360fb5",
   "metadata": {},
   "source": [
    "### Q17. Which version of GPT introduced the concept of few-shot learning? \n",
    "1. GPT \n",
    "2. GPT-2 \n",
    "3. GPT-3 \n",
    "4. GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a63776",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**3. GPT-3** \n",
    "\n",
    "**GPT-3** introduced the concept of **few-shot learning** in a big way. With **175 billion parameters**, it showed that large language models can perform new tasks with:\n",
    "\n",
    "* **Zero-shot learning** → No examples, just instructions.\n",
    "* **One-shot learning** → A single example given.\n",
    "* **Few-shot learning** → A handful of examples provided in the prompt.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **GPT** → The original GPT (2018) only showed standard next-word prediction.\n",
    "2. **GPT-2** → (2019) much larger than GPT, but not famous for few-shot learning.\n",
    "3. **GPT-3** → Correct! (2020) demonstrated strong few-shot and zero-shot capabilities.\n",
    "4. **GPT-4** → Even more advanced, but the *concept* was introduced in GPT-3.\n",
    "\n",
    "Example (Few-shot with GPT-3):\n",
    "**Task:** Translate English → French\n",
    "Prompt:\n",
    "\n",
    "* “Hello → Bonjour”\n",
    "* “Good morning → Bonjour”\n",
    "* “Cat → Chat”\n",
    "  Model then translates new words correctly without explicit training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a701272",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac4ebe34",
   "metadata": {},
   "source": [
    "### Q18. What is a key difference between GPT-2 and GPT-3? \n",
    "1. GPT-2 is bidirectional, while GPT-3 is unidirectional \n",
    "2. GPT-3 has significant more parameters than GPT-2 \n",
    "3. GPT-3 uses a Transformer, while GPT-2 does not \n",
    "4. GPT-2 is pre-trained, while GPT-3 is not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a213bc0",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. GPT-3 has significantly more parameters than GPT-2**\n",
    "\n",
    "Key difference:\n",
    "\n",
    "* **GPT-2 (2019):** ~1.5 billion parameters\n",
    "* **GPT-3 (2020):** ~175 billion parameters\n",
    "\n",
    "That massive increase in scale enabled GPT-3 to demonstrate **few-shot and zero-shot learning**, which GPT-2 could not do effectively.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **GPT-2 is bidirectional, while GPT-3 is unidirectional** → Both are **unidirectional** (causal language models).\n",
    "2. **GPT-3 has significantly more parameters** → Correct! (~175B vs ~1.5B).\n",
    "3. **GPT-3 uses a Transformer, while GPT-2 does not** → Both use the **Transformer decoder** architecture.\n",
    "4. **GPT-2 is pre-trained, while GPT-3 is not** → Both are **pre-trained** on large text corpora.\n",
    "\n",
    "Example:\n",
    "\n",
    "* **GPT-2:** Could generate fluent text but struggled with generalization.\n",
    "* **GPT-3:** Can write essays, translate, summarize, and solve reasoning tasks with little or no fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087c852",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8a14f72",
   "metadata": {},
   "source": [
    "### Q19. What is the primary architecture of the T5 model? \n",
    "1. CNN-based \n",
    "2. RNN-based \n",
    "3. Sequence to sequence with attention \n",
    "4. Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbfd35e",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**3. Sequence to sequence with attention**\n",
    "\n",
    "The **T5 (Text-to-Text Transfer Transformer)** model is built on a **Transformer-based sequence-to-sequence (encoder–decoder) architecture**.\n",
    "\n",
    "* Both **input and output are treated as text**, making every NLP task a text-to-text problem.\n",
    "* Uses **attention mechanisms** in both the encoder and decoder to capture dependencies.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **CNN-based** → T5 is not convolutional.\n",
    "2. **RNN-based** → T5 does not use RNNs; it’s fully Transformer-based.\n",
    "3. **Sequence to sequence with attention** → Correct! Encoder–decoder Transformer.\n",
    "4. **Reinforcement learning** → Not the core architecture (though RL can be used in fine-tuning).\n",
    "\n",
    "Example:\n",
    "\n",
    "* **Task:** Summarization\n",
    "* **Input:** `\"summarize: The cat sat on the mat and looked out the window.\"`\n",
    "* **Output:** `\"The cat watched from the mat.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3830845b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7bfcc4d",
   "metadata": {},
   "source": [
    "### Q20. Which of the following is a significant feature of the T5 model? \n",
    "1. It converts every NLP problem into a text-to-text format \n",
    "2. It is designed for image recognition tasks \n",
    "3. It uses convolutional layers extensively \n",
    "4. It only supports classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aa2f9d",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**1. It converts every NLP problem into a text-to-text format**\n",
    "\n",
    "**T5 (Text-to-Text Transfer Transformer)** is designed to **treat every NLP task as a text-to-text problem**:\n",
    "\n",
    "* **Input:** Always text (e.g., \"translate English to French: Hello\")\n",
    "* **Output:** Always text (e.g., \"Bonjour\")\n",
    "\n",
    "This unified approach allows the same model to handle: translation, summarization, question answering, and classification.\n",
    "\n",
    "Breakdown of options:\n",
    "1. **Converts every NLP problem into text-to-text** → Correct!\n",
    "2. **Designed for image recognition tasks** → T5 is NLP-only.\n",
    "3. **Uses convolutional layers extensively** → Fully Transformer-based.\n",
    "4. **Only supports classification tasks** → Supports many NLP tasks, not just classification.\n",
    "\n",
    "Example:\n",
    "\n",
    "* **Task:** Sentiment analysis\n",
    "* **Input:** `\"sst2 sentence: I love this movie!\"`\n",
    "* **Output:** `\"positive\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e0d4f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5412bde",
   "metadata": {},
   "source": [
    "### Q21. What type of model is Bard primarily based on? \n",
    "1. CNN \n",
    "2. RNN \n",
    "3. Transformer \n",
    "4. LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f594f6",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**3. Transformer**\n",
    "\n",
    "**Bard**, like most modern large language models, is primarily based on the **Transformer architecture**.\n",
    "\n",
    "* Transformers use **self-attention mechanisms** to capture dependencies in sequences efficiently.\n",
    "* This architecture enables Bard to generate coherent text, answer questions, and perform other NLP tasks.\n",
    "\n",
    "Breakdown of options:\n",
    "1. **CNN** → Used mainly for images, not NLP LLMs.\n",
    "2. **RNN** → Older sequential models, replaced by Transformers for LLMs.\n",
    "3. **Transformer** → Correct! Core architecture for Bard.\n",
    "4. **LSTM** → Also a type of RNN; not used in Bard.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Input: `\"Explain quantum computing in simple terms.\"`\n",
    "* Bard uses its Transformer-based model to generate a coherent, context-aware response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399dc17",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af23d16b",
   "metadata": {},
   "source": [
    "### Q22. What distinguishes Bard from other language models? \n",
    "1. Its use of convolutional layers \n",
    "2. Its bidirectional autoregressive nature \n",
    "3. Its focus on image data \n",
    "4. Its reliance on large labelled datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fcfba5",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. Its bidirectional autoregressive nature** \n",
    "\n",
    "**Bard** is designed as a **large language model with bidirectional context** while still being capable of autoregressive text generation:\n",
    "\n",
    "* **Bidirectional** → Can consider context from both left and right (like BERT) for better understanding.\n",
    "* **Autoregressive** → Generates text one token at a time (like GPT), allowing coherent sequence generation.\n",
    "\n",
    "This combination helps Bard generate more context-aware and fluent responses compared to strictly unidirectional models.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **Use of convolutional layers** → Not applicable; Bard is Transformer-based.\n",
    "2. **Bidirectional autoregressive nature** → Correct! Key distinguishing feature.\n",
    "3. **Focus on image data** → Bard is NLP-focused.\n",
    "4. **Reliance on large labelled datasets** → Uses massive text corpora, mostly unlabeled.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Input: `\"Explain photosynthesis.\"`\n",
    "* Bard considers the entire sentence context (both prior and following words in understanding) while generating the answer sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23ac8cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4769594",
   "metadata": {},
   "source": [
    "### Q23. What is a common application of sequence to sequence models besides machine translation? \n",
    "1. Object detection \n",
    "2. Speech recognition \n",
    "3. Image segmentation \n",
    "4. Graph analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cf045",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. Speech recognition** \n",
    "\n",
    "**Sequence-to-sequence (Seq2Seq) models** are widely used for tasks where **both input and output are sequences**, such as:\n",
    "\n",
    "* **Machine translation** (text → text)\n",
    "* **Speech recognition** (audio frames → text)\n",
    "* **Text summarization** (long text → short text)\n",
    "* **Chatbots / dialogue generation**\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **Object detection** → Vision task, not sequence-to-sequence.\n",
    "2. **Speech recognition** → Correct! Converts sequences of audio features into text.\n",
    "3. **Image segmentation** → Not Seq2Seq; typically uses CNNs.\n",
    "4. **Graph analysis** → Different domain; not standard Seq2Seq application.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Input: Audio of someone saying `\"Hello, how are you?\"`\n",
    "* Seq2Seq model output: `\"Hello, how are you?\"` (text transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54867e38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cef3c128",
   "metadata": {},
   "source": [
    "### Q24. What technique is often used to improve the performance of sequence to sequence models? \n",
    "1. Dropout \n",
    "2. Attention mechanism \n",
    "3. Data augmentation \n",
    "4. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dce278",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. Attention mechanism** \n",
    "\n",
    "**Attention mechanisms** are commonly used in **sequence-to-sequence (Seq2Seq) models** to improve performance by allowing the decoder to **focus on relevant parts of the input sequence** when generating each output token.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **Dropout** → Helps prevent overfitting, but not specific to Seq2Seq performance improvement.\n",
    "2. **Attention mechanism** → Correct! Enhances context handling and output accuracy.\n",
    "3. **Data augmentation** → Useful for training, but not a core Seq2Seq improvement technique.\n",
    "4. **Regularization** → Helps generalization, but attention directly improves sequence mapping.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Input: `\"The cat sat on the mat\"`\n",
    "* Without attention: Decoder may miss context for words like `\"mat\"`.\n",
    "* With attention: Decoder focuses on the relevant input word, producing accurate translations or summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc7cc8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6de5582e",
   "metadata": {},
   "source": [
    "### Q25. Which of the following is an advantage of using attention mechanisms in NLP? \n",
    "1. They reduce model complexity \n",
    "2. They help in capturing long-range dependencies \n",
    "3. They decrease training time \n",
    "4. They are only useful for image data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8dd6a",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. They help in capturing long-range dependencies**\n",
    "\n",
    "**Attention mechanisms** allow models to **focus on relevant parts of the input sequence**, regardless of distance, which is especially important in NLP tasks where dependencies can span many words.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **They reduce model complexity** → Attention can actually increase complexity.\n",
    "2. **They help in capturing long-range dependencies** → Correct! Core advantage in NLP.\n",
    "3. **They decrease training time** → Not necessarily; they may increase computation but improve performance.\n",
    "4. **They are only useful for image data** → False; attention is widely used in NLP and other domains.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Sentence: `\"The book that you gave me yesterday was amazing.\"`\n",
    "* Attention allows the model to link `\"book\"` with `\"was\"` even though they are far apart in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3c127",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6de4a96",
   "metadata": {},
   "source": [
    "### Q26. What is the output of an attention mechanism typically used for? \n",
    "1. Scaling the model \n",
    "2. Reducing overfitting \n",
    "3. Adjusting model weights \n",
    "4. Enhancing feature representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde5776a",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**4. Enhancing feature representation**\n",
    "\n",
    "In an **attention mechanism**, the output is a **weighted combination of input features** where the weights reflect the relevance of each input element to the current task. This helps the model **focus on important parts of the input**, enhancing the representation used for decoding or prediction.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **Scaling the model** → Attention does not inherently scale the model.\n",
    "2. **Reducing overfitting** → That’s handled by regularization techniques like dropout.\n",
    "3. **Adjusting model weights** → Weights are updated during training; attention outputs are not directly for weight updates.\n",
    "4. **Enhancing feature representation** → Correct! This is the primary purpose.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Input: `\"The cat sat on the mat\"`\n",
    "* When generating `\"mat\"` in translation, attention emphasizes the word `\"mat\"` in the input sequence, improving the decoder’s feature representation for accurate output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0630c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4767772b",
   "metadata": {},
   "source": [
    "### Q27. Why is self-attention crucial in models like Transformer? \n",
    "1. It processes input sequences independently \n",
    "2. It allows parallel processing of sequence elements \n",
    "3. It reduces model parameters \n",
    "4. It simplifies the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2152594",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. It allows parallel processing of sequence elements**\n",
    "\n",
    "**Self-attention** in Transformers is crucial because it enables each token in a sequence to **attend to all other tokens simultaneously**, allowing the model to capture dependencies **regardless of distance** and to **process all tokens in parallel**, unlike RNNs which are sequential.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **It processes input sequences independently** → Self-attention considers **all tokens together**, not independently.\n",
    "2. **It allows parallel processing of sequence elements** → Correct! This is a key advantage of Transformers.\n",
    "3. **It reduces model parameters** → Self-attention may actually increase computation.\n",
    "4. **It simplifies the training process** → Training is still complex; parallelism improves efficiency but doesn’t simplify training inherently.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Input: `\"The quick brown fox jumps over the lazy dog\"`\n",
    "* Self-attention lets the model understand relationships like `\"fox\"` ↔ `\"jumps\"` and `\"dog\"` ↔ `\"lazy\"` **in parallel**, improving contextual understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c548902",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac7e81c1",
   "metadata": {},
   "source": [
    "### Q28. WHat does self-attention help capture in a sequence? \n",
    "1. Only the first and last tokens \n",
    "2. Relationships between all tokens in the sequence\n",
    "3. Only the most frequent tokens \n",
    "4. Tokens with the highest frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94af7114",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. Relationships between all tokens in the sequence** \n",
    "\n",
    "**Self-attention** allows each token in a sequence to **consider and weigh all other tokens**, capturing dependencies and contextual relationships across the entire sequence—regardless of distance.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **Only the first and last tokens** → Self-attention considers **all tokens**, not just endpoints.\n",
    "2. **Relationships between all tokens** → Correct! This is the core function of self-attention.\n",
    "3. **Only the most frequent tokens** → Frequency doesn’t determine attention; relevance does.\n",
    "4. **Tokens with the highest frequency** → Same as above; attention is context-driven, not frequency-driven.\n",
    "\n",
    "Example:\n",
    "Sentence: `\"The cat that chased the mouse was tired.\"`\n",
    "\n",
    "* Self-attention links `\"cat\"` ↔ `\"was tired\"` even though they are far apart, capturing meaningful relationships throughout the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df358214",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "651d7efe",
   "metadata": {},
   "source": [
    "### Q29. In Transformer models, what does the 'T' in GPT stand for? \n",
    "1. Tree \n",
    "2. Translation \n",
    "3. Transformer \n",
    "4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ec7bf",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**3. Transformer** \n",
    "\n",
    "In **GPT (Generative Pre-trained Transformer)**, the **'T'** stands for **Transformer**, which is the core architecture used in GPT models.\n",
    "\n",
    "* Transformers use **self-attention** mechanisms to model dependencies in sequences efficiently.\n",
    "* GPT specifically uses the **decoder stack** of the Transformer for autoregressive text generation.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **Tree** → Not related to GPT.\n",
    "2. **Translation** → GPT can do translation, but ‘T’ doesn’t stand for this.\n",
    "3. **Transformer** → Correct! Core architecture.\n",
    "4. **Training** → Not correct; pre-training is part of GPT, but not the ‘T’.\n",
    "\n",
    "Example:\n",
    "\n",
    "* GPT uses the Transformer decoder to predict the next word in a sentence:\n",
    "  Input: `\"The weather today is\"` → GPT predicts `\"sunny\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73727cbd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb5b1962",
   "metadata": {},
   "source": [
    "### Q30. Which component in a Transformer model is primarily responsible for allowing it to process sequences in parallel? \n",
    "1. RNN cells \n",
    "2. Self-attention \n",
    "3. Convolutional layers \n",
    "4. Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a38b483",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. Self-attention** \n",
    "\n",
    "In a **Transformer**, **self-attention** allows each token in a sequence to **attend to all other tokens simultaneously**, which enables **parallel processing** of the entire sequence. Unlike RNNs, which process tokens sequentially, Transformers can handle all tokens at once.\n",
    "\n",
    "Breakdown of options:\n",
    "\n",
    "1. **RNN cells** → Sequential processing; prevents parallelization.\n",
    "2. **Self-attention** → Correct! Enables parallel computation.\n",
    "3. **Convolutional layers** → Not used for sequence modeling in Transformers.\n",
    "4. **Positional encoding** → Provides token order information but does not enable parallelism.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Input sequence: `\"The quick brown fox\"`\n",
    "* Self-attention allows the model to compute relationships between `\"The\"`, `\"quick\"`, `\"brown\"`, and `\"fox\"` **all at once**, rather than one at a time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd59a95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80352f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c681efc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53e214",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
